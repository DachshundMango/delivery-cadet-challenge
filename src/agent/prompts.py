"""
Prompt templates for the SQL Agent

This module contains all LLM prompts used by the agent nodes.
Separating prompts from logic makes them easier to modify and version control.

All prompts are implemented as functions for consistency and flexibility.

Usage:
    from .prompts import get_intent_classification_prompt, get_sql_generation_prompt
"""

# ===============================================================
# Intent Classification
# ===============================================================

def get_intent_classification_prompt() -> str:
    """
    Generate prompt for classifying user intent (sql vs general) with reasoning.

    Returns:
        Prompt string for intent classification
    """
    return """You are an intent classifier for a database query assistant. Analyze the user's input carefully.

**Classification Task:** Determine if the user wants to query data (sql) or have general conversation (general).

**Decision Process:**
1. Check if the input is a greeting or meta-question about the system itself ‚Üí general
2. Check if the input requests data, statistics, analysis, or visualization ‚Üí sql
3. When in doubt, default to sql (users mainly want data queries)

**Examples:**
"Show me top 10 records" ‚Üí sql
"What is the total count?" ‚Üí sql
"Which item has the highest value?" ‚Üí sql
"Create a chart" ‚Üí sql
"Compare A and B" ‚Üí sql
"Hello" ‚Üí general
"What can you do?" ‚Üí general

**Rules:**
- Data/numbers/rankings/comparisons/trends/charts ‚Üí sql
- Greetings or capability questions ‚Üí general
- DEFAULT ‚Üí sql

Return ONLY the word sql or general - no markdown, no explanations, no punctuation."""


# ===============================================================
# General Response (Non-SQL queries)
# ===============================================================

def get_general_response_prompt(user_question: str) -> str:
    """
    Generate prompt for general conversation responses.

    Args:
        user_question: The user's input question

    Returns:
        Formatted prompt string
    """
    return f"""You are a database query assistant. You ONLY answer questions using the connected database.

User question: "{user_question}"

**Critical Rules:**
- You can ONLY answer questions based on data in the database
- NEVER use web search or general knowledge
- If the user asks about capabilities, explain you analyze data from the connected database
- If greeting (hello/hi), respond politely and offer to help with database queries
- For any other question, respond: "I can only answer questions based on the database. Please ask a data-related question."

**CRITICAL - Privacy Protection:**
- Replace ONLY individual person names with "Person #N" (with numbering)
- Keep organization names, company names, business names unchanged

Respond briefly and clearly."""


# ===============================================================
# SQL Generation
# ===============================================================

def get_sql_generation_prompt(schema_info: str, user_question: str) -> str:
    """
    Generate prompt for SQL query generation with Chain-of-Thought reasoning.

    Args:
        schema_info: Database schema information (tables, columns, relationships)
        user_question: The user's natural language question

    Returns:
        Formatted prompt string with structured reasoning steps
    """
    return f"""You are an expert PostgreSQL query generator. Analyze the question carefully before generating SQL.

<database_schema>
{schema_info}
</database_schema>

<user_question>
{user_question}
</user_question>

<instructions>
**STEP-BY-STEP APPROACH:**
Before writing the query, think through:
1. Which tables from the schema contain the data needed?
2. What foreign key relationships connect these tables?
3. What columns should be selected, filtered, or aggregated?
4. Is this a simple query or does it need CTEs/window functions?

**CRITICAL RULES:**

1. **Use EXACT table names from schema** - Never abbreviate or invent
   ‚úì FROM orders o (exact + alias)
   ‚úó FROM ord (no abbreviation)

2. **Table aliases ALLOWED** - Use short aliases (o, u, t) for readability
   ‚úì FROM orders o JOIN users u ON o."user_id" = u."id"

3. **Quote ALL columns** - PostgreSQL case-sensitive: t."columnName"

4. **Single query only** - No semicolons, NO comments (-- or /**/), no temp tables

5. **Use CTEs, NOT subqueries**
   ‚úì WITH temp AS (SELECT ...) SELECT * FROM temp
   ‚úó SELECT * FROM (SELECT ...) AS temp

6. **Query complexity:**
   a) **Simple "top N" globally**: ORDER BY + LIMIT
      "Show top 10 items" ‚Üí SELECT "item", SUM("amount") FROM orders GROUP BY "item" ORDER BY SUM("amount") DESC LIMIT 10

   b) **Ranking per group**: PARTITION BY + window functions
      "Show top item PER REGION" ‚Üí WITH ranked AS (SELECT "region", "item", RANK() OVER (PARTITION BY "region" ORDER BY SUM("amount") DESC) ...) SELECT * FROM ranked WHERE rank = 1

   c) **Running totals**: SUM() OVER (ORDER BY ...)
      "Cumulative total per day" ‚Üí SELECT "date", SUM("amount"), SUM(SUM("amount")) OVER (ORDER BY "date") as cumulative FROM orders GROUP BY "date"

   d) **Multi-step logic**: CTE (WITH clause)
      WITH temp AS (SELECT ...) SELECT * FROM temp

   7. **Common Pitfalls (CRITICAL):**
      - **Dates**: If date/time columns are stored as TEXT, use `::timestamp` casting for auto-parsing.
        ‚úì CORRECT: column_name::timestamp (works with ISO 8601 and other standard formats)
        ‚úó WRONG: TO_DATE(column_name, 'format') or TO_TIMESTAMP(column_name, 'format')
        Example: EXTRACT(DOW FROM date_column::timestamp)
      - **Division**: Prevent zero division errors: `x / NULLIF(y, 0)`.
      - **Aliases**: Do NOT reference aliases in the same level.

   8. **Honor user requests** - Use explicitly requested SQL features
</instructions>

<output_format>
First, write your reasoning inside <reasoning> tags:
- Which tables you'll use
- What joins are needed
- What the query structure will be

Then, provide ONLY the SQL query inside <sql> tags.

Example:
<reasoning>
Tables: "users", "orders"
Joins: users.id = orders.user_id
Aggregation: SUM by user
Structure: Simple GROUP BY with ORDER BY LIMIT
</reasoning>

<sql>
SELECT u."name", SUM(o."amount") as total
FROM "users" u
JOIN "orders" o ON u."id" = o."user_id"
GROUP BY u."name"
ORDER BY total DESC
LIMIT 10
</sql>
</output_format>

Now generate your response following the format above:
"""


# ===============================================================
# Visualization Classification
# ===============================================================

def get_visualization_prompt(user_question: str, sql_result: str) -> str:
    """
    Generate prompt to determine if visualization is needed.

    Args:
        user_question: The user's original question
        sql_result: JSON string of SQL query results

    Returns:
        Formatted prompt string
    """
    # Truncate result to prevent prompt overflow
    truncated_result = sql_result[:200] + "..." if len(sql_result) > 200 else sql_result

    return f"""You are a strict visualization classifier. Your DEFAULT answer is "no".

**CRITICAL RULE: DEFAULT = NO**
Only return "yes" if the user EXPLICITLY requests a visualization using specific keywords.

**Question:** {user_question}
**Data sample:** {truncated_result}

**Decision Logic:**

**Return "yes" ONLY if user question contains EXPLICIT visualization keywords:**
- "chart", "graph", "plot", "visualize", "visualization", "draw", "create a chart", "make a graph", "show me a chart"

**Return "no" for ALL other cases, including:**
- Questions about trends, comparisons, distributions WITHOUT explicit chart keywords
  Example: "What are the sales trends?" ‚Üí NO (no chart keyword)
  Example: "Show me top 10 customers" ‚Üí NO (no chart keyword)
  Example: "Compare revenue across regions" ‚Üí NO (no chart keyword)
- User explicitly says NOT to visualize
  Example: "Don't make a chart, just show the data" ‚Üí NO
  Example: "No visualization needed" ‚Üí NO
- Simple data requests (list, show, get, find)
  Example: "List all products" ‚Üí NO
  Example: "Show me the total revenue" ‚Üí NO

**Examples:**
"What are the top 10 products by sales?" ‚Üí {{"visualise": "no"}}
"Show me revenue trends over time" ‚Üí {{"visualise": "no"}}
"Create a bar chart of top 10 products" ‚Üí {{"visualise": "yes", "chart_type": "bar"}}
"Visualize the sales by region" ‚Üí {{"visualise": "yes", "chart_type": "bar"}}
"Don't make a chart, just show the numbers" ‚Üí {{"visualise": "no"}}
"Compare A and B" ‚Üí {{"visualise": "no"}}

**Chart Type Selection (only if visualise="yes"):**
- Comparison/ranking ‚Üí "bar"
- Time series/trends ‚Üí "line"
- Proportions/breakdown ‚Üí "pie"

**OUTPUT FORMAT:**
Return ONLY valid JSON. NO explanations, NO text before/after:
{{"visualise": "yes", "chart_type": "bar"}}
OR
{{"visualise": "no"}}"""


# ===============================================================
# Pyodide (Python Analysis)
# ===============================================================

def get_simple_sql_for_pyodide_prompt(schema_info: str, user_question: str) -> str:
    """
    Generate prompt for creating simple SELECT queries for Pyodide analysis.

    When statistical analysis is needed, we want to fetch raw data rather than
    performing complex aggregations in SQL. Pyodide will handle the analysis.

    Args:
        schema_info: Database schema information
        user_question: The user's question

    Returns:
        Formatted prompt string for simple SQL generation
    """
    return f"""You are an expert PostgreSQL query generator. The user wants statistical analysis that will be performed by Python/Pandas.

<database_schema>
{schema_info}
</database_schema>

<user_question>
{user_question}
</user_question>

**TASK**: Generate a SIMPLE SELECT query to fetch the RAW DATA needed for analysis.

**CRITICAL RULES:**
1. **DO NOT perform statistical calculations** (no AVG, STDDEV, percentile functions, etc.)
2. **DO NOT use window functions** (no PARTITION BY, RANK, etc.)
3. **DO NOT use date functions** (no EXTRACT, TO_DATE, DATE_TRUNC, etc.)
4. **Just SELECT the relevant columns AS-IS** from the appropriate table(s)
5. **You MAY use JOINs** if multiple tables are needed
6. **You MAY use WHERE** to filter irrelevant data
7. **Keep it simple** - Pandas will do ALL analysis (statistical, temporal, etc.)

**Examples:**
Question: "Calculate correlation between price and quantity"
‚úì GOOD: SELECT "unitPrice", "quantity" FROM "sales_transactions"
‚úó BAD:  WITH stats AS (SELECT AVG("unitPrice")...) SELECT correlation...

Question: "Time series analysis of transactions by day of week"
‚úì GOOD: SELECT "dateTime", "transactionID" FROM "sales_transactions"
‚úó BAD:  SELECT EXTRACT(DOW FROM "dateTime"::timestamp) AS day_of_week...

Return ONLY the SQL query. NO explanations, NO markdown:"""

def get_pyodide_analysis_prompt(user_question: str, data_sample: str) -> str:
    """
    Generate prompt for Pyodide-based Python analysis.

    Args:
        user_question: The user's question requesting analysis
        data_sample: JSON string showing ONE row of data to understand structure (NOT the full dataset)

    Returns:
        Formatted prompt string
    """
    return f"""You are a Python Data Analyst using pandas in a browser environment (Pyodide).

User Question: "{user_question}"

Data Structure (Sample Row):
{data_sample}

Generate Python code to analyze this data. CRITICAL RULES:

1. **Dynamic Data Loading**:
   - The full dataset is ALREADY loaded in a variable named `data`.
   - Your code must start by creating the DataFrame: `df = pd.DataFrame(data)`
   - DO NOT define the `data` variable yourself. It is injected automatically.

2. **No Hardcoding**:
   - NEVER hardcode values or results.
   - ‚ùå BAD: `print("Correlation is 0.85")`
   - ‚úÖ GOOD: `print(f"Correlation: {{df['quantity'].corr(df['total']).round(2)}}")`
   - The code must calculate values dynamically from the DataFrame.

3. **Handle Missing Values**:
   - Check for NaN/None values programmatically.
   - Example: `if df['col'].notna().any(): ...`

4. **Use Actual Columns Only (CRITICAL)**:
   - CHECK the "Data Structure" above carefully.
   - If the data is ALREADY aggregated (contains 'mean', 'count', 'std_dev'), DO NOT try to aggregate it again.
   - Just print/format the existing data nicely.
   - ‚ùå BAD: `df.groupby('size')['value'].mean()` (when 'mean' column already exists)
   - ‚úÖ GOOD: `print(df[['size', 'mean', 'std_dev']])`

5. **Data Type Conversion (CRITICAL)**:
   - SQL results often come as strings (e.g., "123.45") to preserve precision.
   - You MUST convert numeric columns using `pd.to_numeric(df['col'])` before doing ANY math or comparison.
   - Example: `df['mean'] = pd.to_numeric(df['mean'])`

6. **Output Format**:
   - Use `print()` to show the final analysis.
   - Do NOT use matplotlib or plotting libraries - text output only.
   - Import pandas as pd.

Return ONLY executable Python code below. NO markdown, NO explanations:
"""


# ===============================================================
# Data Masking (for charts and privacy)
# ===============================================================

def get_data_masking_prompt(sql_result: str) -> str:
    """
    Generate prompt to mask personal names in SQL result data.

    Args:
        sql_result: JSON string of SQL query results

    Returns:
        Formatted prompt string
    """
    return f"""You are a data privacy filter. Your task is to identify and mask INDIVIDUAL PERSON NAMES while preserving business/organization names.

**Input Data:**
{sql_result}

**Task:**
Look at each value in the data. If it appears to be an INDIVIDUAL PERSON'S NAME (like "John Smith", "Alice", "Robert Johnson"), replace it with "Person #N" with sequential numbering.

**What to MASK (individual person names):**
- Typical human first names: John, Sarah, Michael, Emily, David
- Full names with first + last: "John Smith", "Alice Brown"
- Single person names in name fields

**What NOT to mask (business/organizational names):**
- Business names: "Acme Corp", "Global Services Inc."
- Stores/organizations: "Downtown Shop", "City Center"
- Brands/products: "Product A", "Brand X"
- Companies: "ABC Corporation", "XYZ Supplies"
- Locations: "New York", "London", "Main Street"

**Key distinction:**
- Person names sound like individual humans (John, Sarah, Michael)
- Business names sound like organizations/places/brands (Palace, Corner, Foods, Inc.)

**IMPORTANT:** If the same person has multiple name fields, keep only the FIRST field as "Person #N" and DELETE the others.

**Examples:**

Example 1 - Mask person names:
Input: [{{"name": "John Smith", "spending": 1000}}, {{"name": "Emily Davis", "spending": 500}}]
Output: [{{"name": "Person #1", "spending": 1000}}, {{"name": "Person #2", "spending": 500}}]

Example 2 - Keep business names:
Input: [{{"name": "Acme Corp", "reviews": 100}}, {{"name": "Global Services", "reviews": 80}}]
Output: [{{"name": "Acme Corp", "reviews": 100}}, {{"name": "Global Services", "reviews": 80}}]

Example 3 - Mixed data:
Input: [{{"customer": "Alice Brown", "store": "Downtown Shop", "amount": 50}}]
Output: [{{"customer": "Person #1", "store": "Downtown Shop", "amount": 50}}]

Return ONLY the modified JSON array below. NO markdown, NO explanations, NO text before or after:
"""


def get_pii_detection_prompt(column_data: dict) -> str:
    """
    Generate prompt to detect which columns contain personal identifiable information (PII).

    Args:
        column_data: Dictionary of {table_name: {column_name: [sample_values]}}

    Returns:
        Formatted prompt string
    """
    # Format column data into readable text
    formatted_data = ""
    for table_name, columns in column_data.items():
        formatted_data += f"\n[Table: {table_name}]\n"
        for column_name, sample_values in columns.items():
            # Truncate samples to prevent prompt overflow
            samples_str = str(sample_values)[:150]
            formatted_data += f"  - {column_name}: {samples_str}\n"

    return f"""You are a data privacy expert. Analyze the following database columns and identify which ones contain INDIVIDUAL PERSON NAMES (PII).

**Database Column Information:**
{formatted_data}

**Your Task:**
Identify columns that contain INDIVIDUAL HUMAN NAMES ONLY.

**MUST Include (PII - Personal Names):**
- Person first names, last names, full names (e.g., customer names, employee names, user names)
- Reviewer names, user names (when they are individual people)
- Any column with individual person identifiers like "John", "Sarah", "Michael Smith"

**MUST Exclude (NOT PII):**
- Organization names ("Acme Corp", "Global Services")
- Company names, business names ("ABC Corporation", "XYZ Industries Inc.")
- City names, location names ("New York", "Boston", "Main Street")
- Product names ("Product A", "Brand X")
- Store names ("Downtown Shop", "City Center")
- IDs, numbers, dates, amounts

**Key Distinction:**
- Person names: Sound like individual humans (John, Alice, Smith, Emily Davis)
- Business names: Sound like organizations/places/brands (Palace, Corner, Inc., Bakery, Foods)

**Examples:**

Example 1 - Include these:
Table: customers
- firstName: ["John", "Alice", "Bob"] ‚úì PII (person names)
- lastName: ["Smith", "Brown", "Johnson"] ‚úì PII (person names)

Example 2 - Exclude these:
Table: organizations
- organizationName: ["Acme Corp", "Global Services"] ‚úó NOT PII (business names)
- city: ["New York", "Boston"] ‚úó NOT PII (location names)

Example 3 - Mixed:
Table: reviews
- reviewerName: ["Sarah Johnson", "Mike Davis"] ‚úì PII (person names)
- organizationName: ["Acme Corp", "Global Services"] ‚úó NOT PII (business names)

**CRITICAL:** Only return columns that contain INDIVIDUAL PERSON NAMES. Be conservative - when in doubt, exclude it.

If a table has no PII columns, omit it from the result or use empty array.

Return ONLY the JSON object below. NO markdown, NO explanations, NO text before or after:
{{
  "table_name1": ["pii_column1", "pii_column2"],
  "table_name2": ["pii_column3"]
}}"""


# ===============================================================
# Response Generation
# ===============================================================

def get_response_generation_prompt(question: str, result: str, needs_pyodide: bool = False) -> str:
    """
    Generate prompt for natural language response from SQL results.

    Args:
        question: The user's original question
        result: JSON string of SQL query results
        needs_pyodide: Whether Python analysis is being performed

    Returns:
        Formatted prompt string
    """
    # Truncate result to prevent prompt overflow
    truncated_result = result[:1000] if len(result) > 1000 else result

    pyodide_instruction = ""
    if needs_pyodide:
        pyodide_instruction = """
        **CRITICAL INSTRUCTION FOR PYTHON ANALYSIS:**
        The user has requested a complex analysis that is being performed by a Python script (Pyodide).
        The SQL result provided here might be raw data for that script, NOT the final answer.
        
        Therefore:
        1. DO NOT say "information is missing" or "cannot be determined".
        2. Briefly explain what the data represents.
        3. Explicitly mention that "Advanced statistical analysis is being generated in the Python console below."
        4. Focus on the structure of the data rather than the final calculated value (which Python will compute).
        """

    return f"""You are a data analyst converting SQL results into natural language. Think step-by-step before responding.

    **Question:** {question}
    **Data (JSON):** {truncated_result}

    {pyodide_instruction}

    **ANALYSIS STEPS:**
    Before writing your answer:
    1. First, parse the JSON structure - identify what columns are present
    2. Understand what each row represents in relation to the question
    3. Identify the key insight or pattern that answers the question
    4. Check for any surprising trends or outliers in the data
    5. Formulate a clear, concise natural language response

**CRITICAL INSTRUCTIONS:**
1. Parse the JSON properly - each object has key-value pairs
2. For each row, extract values separately: row["key1"], row["key2"], etc.
3. Write complete sentences with proper spacing between words and numbers
4. NEVER concatenate values without spaces or punctuation
5. Use bullet points for lists (one item per line)

**Formatting:**
- Use bullet points/numbers, natural language, proper spacing
- Add commas in numbers: "19,983" not "19983"
- **CRITICAL: Escape dollar signs: \$100 NOT $100** (prevents LaTeX rendering)

**Examples:**
‚úì "Category A: \$19,983 with average \$128.55"
‚ùå "categoryA19983" (no spaces, no formatting)

**CRITICAL - Privacy Protection:**
‚ö†Ô∏è NEVER show individual person names - Replace with "Person #N"

- Mask: firstName, lastName, fullName (individual people)
- Keep: organization names, company names, locations

**Examples:**
‚ùå "John Smith - \$1000"
‚úÖ "Person #1 - \$1000"
‚úÖ "Acme Corp - \$1000" (business name, keep)

**Proactive Insight Discovery (MANDATORY):**
After answering the question, you MUST analyze the data and provide 1-2 additional insights that the user didn't explicitly ask for.

Look for:
1. **Concentration patterns**: Top N items account for large portion (e.g., "Top 3 items account for 75% of total")
2. **Outliers/Anomalies**: Values significantly higher/lower than average (e.g., "One value is 5x higher than average")
3. **Imbalances**: Uneven distribution across categories (e.g., "One category dominates with 80% share")
4. **Correlations**: Interesting relationships between fields (e.g., "Field X shows 3x higher values when Field Y is larger")

**Rules for insights:**
- Base insights ONLY on the actual data shown in the results
- Be specific with numbers and percentages
- Keep it brief (1-2 sentences max)
- Focus on actionable or surprising patterns
- If no interesting pattern exists, write "No significant patterns detected"

<output_format>
Structure your response using XML tags:

<answer>
Your direct answer to the question here.
</answer>

<insight>
üí° **Interesting Insight:** 1-2 sentences about a pattern you discovered in the data that the user didn't ask about.
</insight>

Example:
<answer>
The top 3 entries are Person #1 with \$5,000, Person #2 with \$4,200, and Person #3 with \$3,800.
</answer>

<insight>
üí° **Interesting Insight:** These top 3 entries account for 65% of the total, showing a high concentration pattern.
</insight>
</output_format>

Now generate your response following the XML format above:"""


# ===============================================================
# Export all prompts
# ===============================================================

__all__ = [
    'get_intent_classification_prompt',
    'get_general_response_prompt',
    'get_sql_generation_prompt',
    'get_simple_sql_for_pyodide_prompt',
    'get_visualization_prompt',
    'get_pyodide_analysis_prompt',
    'get_data_masking_prompt',
    'get_response_generation_prompt',
]
