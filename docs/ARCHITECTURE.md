# System Architecture

This document provides a comprehensive overview of Delivery Cadet's system design, component structure, and data flow.

## Table of Contents

- [Features](#features)
- [Tech Stack](#tech-stack)
- [Project Structure](#project-structure)
- [High-Level Architecture](#high-level-architecture)
- [LangGraph Workflow](#langgraph-workflow)
- [Key Components](#key-components)
- [Data Flow](#data-flow)
- [Dataset-Agnostic Design](#dataset-agnostic-design)
- [Module Responsibilities](#module-responsibilities)

---

## Features

### Core Capabilities

- **Natural Language to SQL**: Converts user questions into valid PostgreSQL queries
- **Intent Classification**: Intelligently routes between SQL-based queries and general conversation
- **Automatic Query Retry**: Self-correcting mechanism for failed queries with LLM-powered error feedback
- **Data Visualisation**: Automatic chart generation (bar, line, pie, scatter, area) using Plotly for visual data insights
- **In-Browser Python Execution**: Pyodide-powered pandas analysis running directly in the browser for advanced statistical operations
- **Conversational Interface**: ChatGPT-style UI for seamless user interaction with streaming responses
- **Real-time Streaming**: Live response streaming through the web interface via LangGraph Server
- **Dataset-Agnostic Design**: Easily adaptable to new datasets via metadata configuration without code changes

### Data Pipeline

- **Automated ETL**: Robust CSV-to-database loading with automatic schema generation
- **Primary/Foreign Key Management**: Automatic key detection and relationship mapping with interactive configuration
- **Data Integrity Validation**: Built-in checks for referential integrity and constraint violations
- **Schema Profiling**: Automatic column analysis and statistics generation for optimal query planning
- **Relationship Discovery**: Intelligent FK relationship suggestions based on naming patterns and data analysis
- **Interactive Data Transformation**: SQL console for fixing data issues before loading

### Privacy & Security

- **LLM-Powered PII Detection**: Automatic identification of personal information columns during schema generation
- **Runtime PII Masking**: Personal names automatically replaced with `Person #1`, `Person #2`, etc. in query results
- **Human-in-the-Loop Verification**: Color-coded PII report for user review before masking activation
- **Manual Override**: Edit `schema_info.json` to add/remove PII columns as needed
- **SQL Injection Prevention**: Query validation blocks dangerous keywords (DROP, DELETE, UPDATE, etc.)
- **Read-Only Access**: Only SELECT queries allowed, no write operations
- **Execution Tracing**: LangSmith integration for debugging and monitoring

---

## Tech Stack

### Backend

- **Python 3.12**: Core application language
- **LangGraph**: State machine framework for agent workflow orchestration
- **Cerebras (llama-3.3-70b)**: Fast LLM inference for all AI tasks
- **PostgreSQL 15**: Relational database with JSON support
- **SQLAlchemy**: ORM and connection pooling
- **Plotly**: Interactive chart generation

### Frontend

- **Next.js 15**: React framework with App Router
- **React 19**: UI framework with concurrent rendering
- **TypeScript**: Type-safe JavaScript
- **Tailwind CSS**: Utility-first styling
- **Pyodide**: In-browser Python runtime for pandas analysis
- **react-plotly.js**: Plotly charts in React

### Infrastructure

- **Docker Compose**: PostgreSQL and PgAdmin containerization
- **LangGraph Server**: Agent runtime with streaming support
- **LangSmith**: Execution tracing and debugging

---

## Project Structure

```
cadet/
â”œâ”€â”€ src/                          # Python backend source code
â”‚   â”œâ”€â”€ agent/                    # LangGraph agent workflow
â”‚   â”‚   â”œâ”€â”€ __init__.py           # Public API exports
â”‚   â”‚   â”œâ”€â”€ graph.py              # LangGraph workflow definition (76 LOC)
â”‚   â”‚   â”œâ”€â”€ nodes.py              # Agent node implementations (788 LOC)
â”‚   â”‚   â”œâ”€â”€ prompts/              # Modular LLM prompt templates (655 LOC)
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py       # Prompt exports
â”‚   â”‚   â”‚   â”œâ”€â”€ intent.py         # Intent classification & general responses (68 LOC)
â”‚   â”‚   â”‚   â”œâ”€â”€ sql.py            # SQL generation prompts (154 LOC)
â”‚   â”‚   â”‚   â”œâ”€â”€ visualization.py  # Chart & visualization prompts (87 LOC)
â”‚   â”‚   â”‚   â”œâ”€â”€ analysis.py       # Pyodide analysis prompts (63 LOC)
â”‚   â”‚   â”‚   â””â”€â”€ privacy.py        # PII masking & response prompts (245 LOC)
â”‚   â”‚   â”œâ”€â”€ helpers.py            # Reusable utilities (147 LOC)
â”‚   â”‚   â”œâ”€â”€ config.py             # LLM instances & constants (63 LOC)
â”‚   â”‚   â”œâ”€â”€ routing.py            # Conditional routing logic (106 LOC)
â”‚   â”‚   â”œâ”€â”€ feedbacks.py          # Error feedback messages (306 LOC)
â”‚   â”‚   â”œâ”€â”€ error_feedback.py     # Error feedback router (114 LOC)
â”‚   â”‚   â””â”€â”€ state.py              # State management schema (65 LOC)
â”‚   â”‚
â”‚   â”œâ”€â”€ data_pipeline/            # ETL and data preparation (1,606 LOC)
â”‚   â”‚   â”œâ”€â”€ __init__.py           # Public API exports
â”‚   â”‚   â”œâ”€â”€ profiler.py           # CSV data profiler (91 LOC)
â”‚   â”‚   â”œâ”€â”€ relationship_discovery.py  # Automatic FK detection (226 LOC)
â”‚   â”‚   â”œâ”€â”€ integrity_checker.py  # Data validation utilities (261 LOC)
â”‚   â”‚   â”œâ”€â”€ load_data.py          # CSV to DB ETL pipeline (167 LOC)
â”‚   â”‚   â”œâ”€â”€ transform_data.py     # Interactive data transformation (231 LOC)
â”‚   â”‚   â”œâ”€â”€ pii_discovery.py      # LLM-based PII column detection (189 LOC)
â”‚   â”‚   â”œâ”€â”€ generate_schema.py    # Schema + PII metadata generator (226 LOC)
â”‚   â”‚   â””â”€â”€ setup.py              # Automated pipeline orchestrator (190 LOC)
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                     # Shared utilities (588 LOC)
â”‚   â”‚   â”œâ”€â”€ __init__.py           # Public API exports
â”‚   â”‚   â”œâ”€â”€ console.py            # Unified CLI output formatting (72 LOC)
â”‚   â”‚   â”œâ”€â”€ db.py                 # Database connection management (84 LOC)
â”‚   â”‚   â”œâ”€â”€ logger.py             # Logging configuration (44 LOC)
â”‚   â”‚   â”œâ”€â”€ errors.py             # Custom exception classes (55 LOC)
â”‚   â”‚   â””â”€â”€ validation.py         # SQL validation & security (302 LOC)
â”‚   â”‚
â”‚   â”œâ”€â”€ config/                   # Configuration and metadata
â”‚   â”‚   â”œâ”€â”€ keys.json             # PK/FK metadata configuration
â”‚   â”‚   â”œâ”€â”€ schema_info.json      # Generated schema (used by LLM)
â”‚   â”‚   â”œâ”€â”€ schema_info.md        # Human-readable schema docs
â”‚   â”‚   â””â”€â”€ data_profile.json     # Data profiling statistics
â”‚   â”‚
â”‚   â”œâ”€â”€ setup.py                  # Automated pipeline orchestrator
â”‚   â”œâ”€â”€ reset_db.py               # Database + config reset utility
â”‚   â””â”€â”€ cli.py                    # CLI entry point
â”‚
â”œâ”€â”€ frontend/                     # Next.js 15 + React 19 frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/                  # Next.js App Router
â”‚   â”‚   â”œâ”€â”€ components/           # React components
â”‚   â”‚   â”‚   â”œâ”€â”€ plotly-chart.tsx  # Plotly visualization
â”‚   â”‚   â”‚   â””â”€â”€ python-runner.tsx # Pyodide runtime
â”‚   â”‚   â”œâ”€â”€ providers/            # Context providers & LangGraph client
â”‚   â”‚   â””â”€â”€ hooks/                # Custom React hooks
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ tailwind.config.js
â”‚
â”œâ”€â”€ data/                         # CSV data files (8 files)
â”‚   â”œâ”€â”€ sales_customers.csv
â”‚   â”œâ”€â”€ sales_franchises.csv
â”‚   â”œâ”€â”€ sales_suppliers.csv
â”‚   â”œâ”€â”€ sales_transactions.csv
â”‚   â”œâ”€â”€ media_customer_reviews.csv
â”‚   â”œâ”€â”€ media_gold_reviews_chunked.csv
â”‚   â”œâ”€â”€ media_campaigns.csv
â”‚   â””â”€â”€ missing_suppliers.csv
â”‚
â”œâ”€â”€ docs/                         # Documentation
â”‚   â”œâ”€â”€ ERROR-HANDLING.md         # Error handling & retry logic
â”‚   â”œâ”€â”€ DEVELOPMENT.md            # Development guide & contributing
â”‚   â””â”€â”€ ARCHITECTURE.md           # This file
â”‚
â”œâ”€â”€ tests/                        # Testing (672 LOC)
â”‚   â”œâ”€â”€ test_security.py          # Security validation tests (111 LOC)
â”‚   â”œâ”€â”€ agent/                    # Agent module tests
â”‚   â”‚   â”œâ”€â”€ test_routing.py       # Routing logic tests (143 LOC)
â”‚   â”‚   â”œâ”€â”€ test_helpers.py       # Helper utilities tests (185 LOC)
â”‚   â”‚   â””â”€â”€ test_config.py        # Configuration tests (100 LOC)
â”‚   â””â”€â”€ README.md                 # Testing documentation
â”‚
â”œâ”€â”€ docker-compose.yaml           # PostgreSQL + PgAdmin
â”œâ”€â”€ langgraph.json                # LangGraph configuration
â”œâ”€â”€ start.sh                      # One-command startup script
â”œâ”€â”€ environment.yml               # Conda environment (cross-platform)
â”œâ”€â”€ requirements.txt              # Python dependencies (pip)
â”œâ”€â”€ .env                          # Environment variables
â””â”€â”€ README.md                     # User-facing documentation
```

---

## High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Frontend (Browser)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ ChatGPT-style  â”‚  â”‚ Plotly Chart â”‚  â”‚ Pyodide (Python)   â”‚  â”‚
â”‚  â”‚ UI (React 19)  â”‚  â”‚ Renderer     â”‚  â”‚ Runtime (Pandas)   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚           â”‚                  â”‚                  â”‚                 â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                              â”‚                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚ HTTP/WebSocket (Streaming)
                               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              â–¼                                     â”‚
â”‚                    LangGraph Server (Port 2024)                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚              LangGraph State Machine (graph.py)             â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ â”‚
â”‚  â”‚  â”‚Intent     â”‚â†’ â”‚SQL Gen  â”‚â†’ â”‚Validationâ”‚â†’ â”‚Execution    â”‚â”‚ â”‚
â”‚  â”‚  â”‚Classifier â”‚  â”‚(LLM)    â”‚  â”‚(Security)â”‚  â”‚(PostgreSQL) â”‚â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚ â”‚
â”‚  â”‚                       â†“                           â†“          â”‚ â”‚
â”‚  â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚ â”‚
â”‚  â”‚                  â”‚Feedback â”‚â†â”€ Error? â”€â”€â”€â”€â”‚Check     â”‚     â”‚ â”‚
â”‚  â”‚                  â”‚(Retry)  â”‚               â”‚Result    â”‚     â”‚ â”‚
â”‚  â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                              â”‚                                     â”‚
â”‚                              â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                  LLM Provider (Cerebras)                     â”‚ â”‚
â”‚  â”‚              llama-3.3-70b (OpenAI-compatible API)          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     PostgreSQL 15 Database                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Customers  â”‚  â”‚ Orders     â”‚  â”‚ Products   â”‚  â”‚ Reviews   â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                      Foreign Key Constraints                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## LangGraph Workflow

### ğŸ¯ Quick Summary

**How Delivery Cadet answers your questions:**

1. **Question received** â†’ LLM generates SQL â†’ PostgreSQL executes â†’ Results returned
2. **SQL fails?** â†’ Error analysed, automatically retries up to 3 times (LLM receives feedback and fixes it)
3. **All 3 attempts fail?** â†’ Switches to simple SQL to fetch raw data â†’ Python (pandas) processes it in browser

> ğŸ’¡ **Core idea:** If complex SQL doesn't work â†’ Just fetch the data â†’ Let Python handle it in the user's browser

---

### ğŸ“– Real-World Examples

#### Scenario A: Success on First Try âœ…

```
User: "What were last month's sales?"
   â†“
LLM: SELECT SUM(amount) FROM orders WHERE created_at >= '2024-12-01'
   â†“
PostgreSQL executes â†’ Success!
   â†“
Result: "Last month's sales were $125,450."
```

#### Scenario B: Success After Retry ğŸ”„

```
User: "Show purchase count by customer"
   â†“
[Attempt 1] LLM: SELECT customer, COUNT(*) FROM it GROUP BY 1  âŒ
Error: "Unknown tables: {'it'}"
   â†“
Feedback: "Table name error. Available: customers, orders. No abbreviations."
   â†“
[Attempt 2] LLM: SELECT customer_name, COUNT(*) FROM orders GROUP BY 1  âœ…
PostgreSQL executes â†’ Success!
   â†“
Result: Bar chart showing purchases by customer
```

#### Scenario C: Pyodide Fallback ğŸ

```
User: "Analyse sales trends and correlations by date"
   â†“
[Attempts 1-3] Complex SQL with date functions fails 3 times âŒ
   â†“
System: "SQL's not cutting it... switching strategy!"
   â†“
[Attempt 4] Simple SQL: SELECT date, amount FROM orders  âœ…
   â†“
Python executes in browser:
  df = pandas.DataFrame(data)
  df['date'] = pandas.to_datetime(df['date'])
  correlation = df['amount'].corr(df['date'])
   â†“
Result: "The correlation coefficient between sales and date is 0.73."
```

---

### ğŸ¬ Step-by-Step Guide

#### Step 1: Receive Question ğŸ“

```
[read_question]
Extract question from user message
   â†“
state.user_question = "What were last month's sales?"
```

#### Step 2: Classify Intent ğŸ¤”

```
[intent_classification]
LLM analyses question (temperature: 0.0 = consistent)
   â†“
Decision: "sales" = data query required
   â†“
state.intent = "sql"  (or "general")
```

- **"sql"** â†’ Database query needed
- **"general"** â†’ Casual chat (e.g., "hello", "thanks")

#### Step 3: Decide Analysis Method ğŸ”

```
[pyodide_request_classification]
Check for advanced analysis keywords:
  - "correlation", "regression", "standard deviation"
  - "statistics", "distribution", "outliers"
   â†“
None found â†’ SQL alone is sufficient
Found â†’ Python needed (simple SQL + Pandas)
   â†“
state.needs_pyodide = True/False
```

#### Step 4: Generate & Execute SQL ğŸ’»

```
[generate_SQL]
1. Load schema: schema_info.json (cached)
2. Check needs_pyodide flag:
   - False â†’ Complex SQL (JOIN, GROUP BY, aggregations)
   - True â†’ Simple SQL (SELECT columns only)
3. Ask LLM to generate SQL (temperature: 0.1 = accuracy)
4. Security validation: Check for DROP/DELETE/--
   â†“
state.sql_query = "SELECT ..."
   â†“
[execute_SQL]
PostgreSQL executes
   â†“
state.query_result = '[{"col": "val"}]'  (or "Error: ...")
```

**ğŸ”„ If it fails?**  
â†’ See [Retry Mechanism](#retry-mechanism-automatic-recovery)

#### Step 5: Determine if Visualisation Needed ğŸ“Š

```
[visualisation_request_classification]
Analyse question:
  - Contains "chart", "graph", "visualise" keywords?
  - Is data suitable for visualisation?
   â†“
Yes â†’ LLM generates chart title + creates Plotly chart
No â†’ Text response only
   â†“
state.plotly_data = '{"type": "bar", ...}'  (or None)
```

#### Step 6: Python Analysis (If Needed) ğŸ

```
[generate_pyodide_analysis]  (only when needs_pyodide=True)
LLM generates pandas code:
  import pandas as pd
  df = pd.DataFrame(data)
  result = df.groupby('category')['amount'].mean()
   â†“
Pyodide executes in browser
   â†“
Result stored as ToolMessage
```

#### Step 7: Generate Final Response âœï¸

```
[generate_response]
LLM synthesises (temperature: 0.7 = natural):
  - SQL results
  - Chart (if present)
  - Python analysis (if present)
   â†“
Converts to natural language
   â†“
Streams to user
```

---

### ğŸ”„ Retry Mechanism (Automatic Recovery)

#### ğŸ¯ Core Rules

| Situation                | Action                  | Counter   | Next Step           |
| ------------------------ | ----------------------- | --------- | ------------------- |
| SQL succeeds âœ…          | Proceed                 | -         | Visualisation check |
| SQL fails (1-2 times) âŒ | Analyse error â†’ Retry   | +1        | Regenerate SQL      |
| SQL fails (3 times) ğŸ’¥   | Enable Pyodide fallback | Reset â†’ 0 | Simple SQL          |
| Pyodide also fails ğŸš«    | Give up                 | -         | Error message       |

#### ğŸ“– Detailed Flow

```
execute_SQL
   â†“
Check result (routing.py: decide_sql_retry_route)
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Success âœ… â”‚  Failed âŒ   â”‚  None âš ï¸    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚             â”‚             â”‚
       â–¼             â–¼             â–¼
return "success"  is_error_result?  return "retry"
       â”‚             â”‚
       â”‚        â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
       â”‚        â”‚  True   â”‚
       â”‚        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
       â”‚             â”‚
       â”‚        retry_count < 3?
       â”‚             â”‚
       â”‚        â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
       â”‚    Yes â”‚     No  â”‚
       â”‚        â–¼         â–¼
       â”‚    "retry"   fallback_attempted?
       â”‚        â”‚         â”‚
       â”‚        â”‚    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
       â”‚        â”‚  No â”‚    Yes  â”‚
       â”‚        â”‚    â–¼         â–¼
       â”‚        â”‚"fallback" "success"
       â”‚        â”‚    â”‚      (give up)
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
        Route to next node
```

<details>
<summary>ğŸ’¡ Code Implementation Details (Click to expand)</summary>

**State Variables (state.py):**

- `sql_retry_count`: Number of SQL failures (0-3)
- `pyodide_fallback_attempted`: Whether fallback has been tried (True/False)
- `query_result`: Execution result or "Error: ..." string

**Routing Logic (routing.py: decide_sql_retry_route):**

```python
# 1. Result is None â†’ retry
if result is None:
    return "retry"

# 2. Check for errors
if is_error_result(result):  # Checks if starts with "Error:"
    retry_count = state.get('sql_retry_count', 0) or 0

    # Less than 3 attempts â†’ retry
    if retry_count < max_retries:
        return "retry"  # sql_retry_count++ handled in execute_SQL

    # 3 or more attempts â†’ check fallback status
    fallback_attempted = state.get('pyodide_fallback_attempted', False)
    if not fallback_attempted:
        return "fallback"  # Switch to Pyodide mode
    else:
        return "success"  # Give up, pass error message

# 3. Success
return "success"
```

**Enable Fallback (nodes.py: enable_pyodide_fallback):**

```python
state['needs_pyodide'] = True  # Simple SQL mode
state['sql_retry_count'] = 0   # Reset counter
state['query_result'] = None   # Clear error
state['pyodide_fallback_attempted'] = True  # Prevent infinite loop
```

</details>

---

### ğŸ”§ Technical Details

<details>
<summary>ğŸ“ Complete State Machine Diagram (Click to expand)</summary>

### State Machine Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    START    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ read_question   â”‚  Extract user question from messages
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ intent_classificationâ”‚  Classify as "sql" or "general"
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  (Temperature: 0.0 - deterministic)
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
    â”‚             â”‚
    â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚pyodide_request_              â”‚  â”‚generate_general_responseâ”‚
â”‚      classification          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                              â”‚              â”‚ (Temperature: 0.7)
â”‚ Check for analysis keywords  â”‚              â–¼
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”Œâ”€â”€â”€â”€â”€â”
            â”‚                              â”‚ END â”‚
      â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                       â””â”€â”€â”€â”€â”€â”˜
      â”‚            â”‚
      â–¼            â–¼
 [needs_pyodide=True]  [needs_pyodide=False]
      â”‚            â”‚
      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚generate_  â”‚  Uses simple SQL (needs_pyodide=True)
      â”‚   SQL     â”‚  or complex SQL (needs_pyodide=False)
      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  (Temperature: 0.1 - accurate)
            â”‚
            â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚execute_SQLâ”‚
      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
            â”‚
      â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â” (check_query_validation)
      â”‚            â”‚
  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”    â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
  â”‚[retry]â”‚    â”‚[fallback] â”‚  sql_retry_count >= 3
  â”‚       â”‚    â”‚           â”‚  AND !pyodide_fallback_attempted
  â””â”€â”€â”€â”¬â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
      â”‚              â”‚
      â”‚              â–¼
      â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚        â”‚enable_pyodide_      â”‚  Set needs_pyodide=True
      â”‚        â”‚    fallback         â”‚  Reset sql_retry_count=0
      â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  Clear query errors
      â”‚                   â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼ (both retry paths lead back to generate_SQL)

      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚     [success]                â”‚  No errors OR max retries exceeded
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  with fallback already attempted
                 â”‚
                 â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚visualisation_request_        â”‚
      â”‚      classification          â”‚  Determine if chart is needed
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  (Temperature: 0.0 - strict)
                  â”‚
            â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â” (check_pyodide_classification)
            â”‚            â”‚
            â–¼            â–¼
   [needs_pyodide]   [skip]
            â”‚            â”‚
            â–¼            â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
   â”‚generate_pyodide_â”‚  â”‚  Generate pandas analysis code
   â”‚    analysis     â”‚  â”‚  (Injects CSV data)
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
            â”‚            â”‚
            â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚generate_      â”‚
            â”‚  response     â”‚  Format natural language answer
            â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  (Temperature: 0.7 - conversational)
                    â”‚
                    â–¼
                 â”Œâ”€â”€â”€â”€â”€â”
                 â”‚ END â”‚
                 â””â”€â”€â”€â”€â”€â”˜
```

</details>

---

## Key Components

### 1. Intent Classification

- **Node:** `intent_classification`
- **Purpose:** Routes between SQL generation (`"sql"`) and general conversation (`"general"`) using deterministic classification

### 2. SQL Generation

- **Node:** `generate_SQL`
- **Prompt Selection:** Conditional based on `needs_pyodide` flag
  - **Simple SQL** (pyodide=True): Raw data fetch only (no aggregations/functions) for Pandas processing
  - **Complex SQL** (pyodide=False): Full analytical queries with joins, aggregations, window functions
- **Process:** Load cached schema â†’ Select prompt â†’ Add retry feedback if needed â†’ Generate SQL â†’ Validate for security

### 3. SQL Validation

- **Module:** `validation.py`
- **Checks:** Forbidden keywords (DROP/DELETE), multiple statements, comments, unknown table names
- **Security:** Uses `sqlparse` to extract table names while skipping function arguments to prevent false positives

### 4. Query Execution

- **Node:** `execute_SQL`
- **Process:** Execute via SQLAlchemy â†’ Apply deterministic PII masking â†’ Return JSON or error
- **Retry:** Max 3 attempts (tracked via `sql_retry_count`)

### 5. Visualization Request Classification

- **Node:** `visualisation_request_classification`
- **Detection:** Keyword-based (chart, graph, plot, visualize)
- **Chart Types:** bar, line, pie, scatter, area

### 6. Pyodide Request Classification

- **Node:** `pyodide_request_classification`
- **Timing:** Runs BEFORE SQL generation to prevent complex SQL when statistical analysis is needed
- **Detection:** Keyword-based (correlation, statistics, distribution, outliers, etc.)
- **Output:** Sets `needs_pyodide` flag to trigger simple SQL mode

### 7. Chart Generation

- **Technology:** Plotly.js + react-plotly.js
- **Process:** Determine chart type â†’ Extract axes from SQL columns â†’ Generate dynamic title (60 char) â†’ Return Plotly JSON spec

### 8. In-Browser Python Execution

- **Technology:** Pyodide (WebAssembly) + pandas
- **Process:** LLM generates pandas code â†’ Data injected as CSV â†’ Pyodide executes in browser sandbox
- **Security:** No server-side code execution

### 9. Pyodide Fallback Mechanism

- **Node:** `enable_pyodide_fallback`
- **Trigger:** After 3 consecutive SQL failures
- **Strategy:** Switch from complex SQL to simple SELECT + Pandas analysis in browser
- **Implementation:** Sets `needs_pyodide=True`, resets `sql_retry_count=0`, prevents infinite loops via `pyodide_fallback_attempted` flag

### 10. Response Generation

- **Node:** `generate_response`
- **Output:** Natural language synthesis of SQL results, charts, and Python analysis with real-time streaming

### 11. Error Feedback System

**Two-layer architecture for targeted error correction:**

| Module                | Responsibility                                                    | Example                                                              |
| --------------------- | ----------------------------------------------------------------- | -------------------------------------------------------------------- |
| **error_feedback.py** | Analyzes error messages via regex, routes to appropriate feedback | Detects "unknown table 'it'" â†’ calls `get_unknown_tables_feedback()` |
| **feedbacks.py**      | Stores error-specific message templates (9 types)                 | Returns: "Use ONLY: customers, orders. No abbreviations."            |

**Feedback Types:** Unknown tables, multiple statements, SQL comments, forbidden keywords, column not found, division by zero, datetime format, alias reference, parsing errors

---

## Data Flow

### User Question â†’ SQL Result

```
1. User Input
   "Show me top 5 customers by total spending"

2. Intent Classification (LLM)
   â†’ intent: "sql"

3. SQL Generation (LLM + Schema)
   schema_info.json â†’ Prompt
   â†’ SQL: SELECT c."name", SUM(o."amount") as total FROM customers c JOIN orders o...

4. SQL Validation (validation.py)
   âœ“ No dangerous keywords
   âœ“ Single statement
   âœ“ No comments
   âœ“ Tables exist: customers, orders

5. Query Execution (PostgreSQL)
   â†’ Result: [{"name": "Person #1", "total": 15000}, ...]
   â†’ PII masked: "John Smith" â†’ "Person #1"

6. Visualization Check (LLM)
   â†’ visualise: "no" (no chart keywords in question)

7. Pyodide Check (Keyword)
   â†’ needs_pyodide: false

8. Response Generation (LLM)
   â†’ Answer: "The top 5 customers by total spending are..."
   â†’ Insight: "Person #1 accounts for 30% of total revenue..."

9. Frontend Display
   â†’ Streaming text response to user
```

---

## Dataset-Agnostic Design

### Core Principle

**All table/column information is loaded from `schema_info.json` at runtime, not hardcoded in prompts.**

### Implementation

1. **Schema Generation** (generate_schema.py)

   - Reads database metadata
   - Detects PII columns via LLM
   - Outputs `schema_info.json`

2. **Runtime Loading** (nodes.py:95-120)

   ```python
   def load_schema_info() -> str:
       global _SCHEMA_CACHE
       if _SCHEMA_CACHE is None:
           with open(SCHEMA_JSON_PATH, 'r') as f:
               schema_data = json.load(f)
           _SCHEMA_CACHE = format_schema_for_prompt(schema_data)
       return _SCHEMA_CACHE
   ```

3. **Prompt Injection** (prompts.py:85-175)

   ```python
   def get_sql_generation_prompt(schema_info: str, user_question: str) -> str:
       return f"""
       <database_schema>
       {schema_info}
       </database_schema>

       <user_question>
       {user_question}
       </user_question>
       """
   ```

### Benefits

- âœ… Swap datasets by replacing CSVs and re-running pipeline
- âœ… No code changes required for new schemas
- âœ… Scalable to different domains (retail, healthcare, finance, etc.)

---

## Module Responsibilities

### agent/ - LangGraph Workflow

- **graph.py:** StateGraph definition, conditional edges, retry logic, Pyodide fallback routing
- **nodes.py:** Node implementations, LLM calls, error handling, PII masking
- **prompts/:** Modular LLM prompt templates organized by function
  - **intent.py:** Intent classification & general conversation responses
  - **sql.py:** SQL generation prompts (complex & simple modes for Pyodide)
  - **visualization.py:** Chart request detection & title generation
  - **analysis.py:** Pyodide analysis code generation prompts
  - **privacy.py:** PII masking & natural language response formatting
- **helpers.py:** Reusable utilities (schema caching, DB engine pooling, PII masking)
- **config.py:** LLM instances with task-specific temperatures & workflow constants
- **routing.py:** Conditional routing logic (RouteDecider class with static methods)
- **feedbacks.py:** Error feedback messages - actual feedback strings for each error type
- **error_feedback.py:** Error feedback router - analyzes errors and routes to appropriate feedback
- **state.py:** TypedDict schema for state management (includes fallback flags and retry counters)

### core/ - Shared Utilities

- **validation.py:** SQL security checks, table name validation
- **db.py:** Database connection pooling (singleton pattern)
- **logger.py:** Structured logging configuration
- **errors.py:** Custom exception hierarchy
- **console.py:** CLI output formatting

### data_pipeline/ - ETL

- **profiler.py:** Analyze CSV structure and statistics
- **relationship_discovery.py:** Suggest FK relationships (interactive)
- **load_data.py:** CSV â†’ PostgreSQL with constraints
- **integrity_checker.py:** Validate PK/FK integrity, detect offsets
- **transform_data.py:** Interactive SQL console for data fixes
- **pii_discovery.py:** LLM-based PII column detection
- **generate_schema.py:** Create schema metadata + PII report
- **setup.py:** Automated pipeline orchestration

---

## Performance Optimizations

### 1. Caching

- **Schema:** Loaded once, cached globally (`_SCHEMA_CACHE`)
- **Database Engine:** Connection pool reused (`_DB_ENGINE`)
- **Frontend:** Chart stability via revision prop and explicit sizing

### 2. Temperature Tuning

- **Intent (0.0):** Deterministic routing
- **SQL (0.1):** Accurate, minimal hallucination
- **Visualization (0.0):** Strict keyword matching
- **Response (0.7):** Natural, varied language

### 3. Streaming

- **LangGraph Server:** Real-time response streaming
- **Frontend:** Progressive UI updates

### 4. Connection Pooling

- **SQLAlchemy:** pool_size=5, max_overflow=10
- **Reuses connections** across requests

---

## Security Layers

### 1. SQL Injection Prevention (validation.py)

- Forbidden keyword blocking
- Multiple statement prevention
- Comment removal
- Table name whitelist validation

### 2. PII Masking (nodes.py:128-194)

- LLM-based detection during schema generation
- Deterministic masking at query execution
- Person names â†’ "Person #N" (sequential)
- Organization names preserved

### 3. Read-Only Access

- Only SELECT queries allowed
- No write operations (INSERT, UPDATE, DELETE)
- No schema modifications (CREATE, ALTER, DROP)

### 4. Rate Limiting

- Max retry limit: 3 attempts
- Prevents infinite loops

---

## Technology Choices

### Why LangGraph?

- **State Management:** Built-in state persistence
- **Conditional Routing:** Easy error handling with conditional edges
- **Streaming:** Native streaming support
- **Debugging:** LangSmith integration for trace visualization

### Why Cerebras (llama-3.3-70b)?

- **Performance:** Fast inference (previously Groq)
- **OpenAI-compatible API:** Easy integration
- **Cost-effective:** Competitive pricing

### Why PostgreSQL?

- **Relational:** Strong FK constraint support
- **JSON Support:** Native JSON column types
- **Mature:** Well-documented, stable

### Why Next.js 15?

- **App Router:** Server components, streaming
- **React 19:** Latest features (concurrent rendering)
- **TypeScript:** Type safety

---

## Related Documentation

- [Error Handling Guide](ERROR-HANDLING.md) - SQL validation, retry logic, debugging
- [Development Guide](DEVELOPMENT.md) - Contributing and extending
- README.md - User setup and usage

---

**Last Updated:** 2026-01-11
