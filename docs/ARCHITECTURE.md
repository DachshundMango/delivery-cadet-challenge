# System Architecture

This document provides a comprehensive overview of Delivery Cadet's system design, component structure, and data flow.

## Table of Contents

- [Features](#features)
- [Tech Stack](#tech-stack)
- [Project Structure](#project-structure)
- [High-Level Architecture](#high-level-architecture)
- [LangGraph Workflow](#langgraph-workflow)
- [Key Components](#key-components)
- [Data Flow](#data-flow)
- [Dataset-Agnostic Design](#dataset-agnostic-design)
- [Module Responsibilities](#module-responsibilities)

---

## Features

### Core Capabilities

- **Natural Language to SQL**: Converts user questions into valid PostgreSQL queries
- **Intent Classification**: Intelligently routes between SQL-based queries and general conversation
- **Automatic Query Retry**: Self-correcting mechanism for failed queries with LLM-powered error feedback
- **Data Visualisation**: Automatic chart generation (bar, line, pie, scatter, area) using Plotly for visual data insights
- **In-Browser Python Execution**: Pyodide-powered pandas analysis running directly in the browser for advanced statistical operations
- **Conversational Interface**: ChatGPT-style UI for seamless user interaction with streaming responses
- **Real-time Streaming**: Live response streaming through the web interface via LangGraph Server
- **Dataset-Agnostic Design**: Easily adaptable to new datasets via metadata configuration without code changes

### Data Pipeline

- **Automated ETL**: Robust CSV-to-database loading with automatic schema generation
- **Primary/Foreign Key Management**: Automatic key detection and relationship mapping with interactive configuration
- **Data Integrity Validation**: Built-in checks for referential integrity and constraint violations
- **Schema Profiling**: Automatic column analysis and statistics generation for optimal query planning
- **Relationship Discovery**: Intelligent FK relationship suggestions based on naming patterns and data analysis
- **Interactive Data Transformation**: SQL console for fixing data issues before loading

### Privacy & Security

- **LLM-Powered PII Detection**: Automatic identification of personal information columns during schema generation
- **Runtime PII Masking**: Personal names automatically replaced with `Person #1`, `Person #2`, etc. in query results
- **Human-in-the-Loop Verification**: Color-coded PII report for user review before masking activation
- **Manual Override**: Edit `schema_info.json` to add/remove PII columns as needed
- **SQL Injection Prevention**: Query validation blocks dangerous keywords (DROP, DELETE, UPDATE, etc.)
- **Read-Only Access**: Only SELECT queries allowed, no write operations
- **Execution Tracing**: LangSmith integration for debugging and monitoring

---

## Tech Stack

### Backend
- **Python 3.12**: Core application language
- **LangGraph**: State machine framework for agent workflow orchestration
- **Cerebras (llama-3.3-70b)**: Fast LLM inference for all AI tasks
- **PostgreSQL 15**: Relational database with JSON support
- **SQLAlchemy**: ORM and connection pooling
- **Plotly**: Interactive chart generation

### Frontend
- **Next.js 15**: React framework with App Router
- **React 19**: UI framework with concurrent rendering
- **TypeScript**: Type-safe JavaScript
- **Tailwind CSS**: Utility-first styling
- **Pyodide**: In-browser Python runtime for pandas analysis
- **react-plotly.js**: Plotly charts in React

### Infrastructure
- **Docker Compose**: PostgreSQL and PgAdmin containerization
- **LangGraph Server**: Agent runtime with streaming support
- **LangSmith**: Execution tracing and debugging

---

## Project Structure

```
cadet/
â”œâ”€â”€ src/                          # Python backend source code
â”‚   â”œâ”€â”€ agent/                    # LangGraph agent workflow
â”‚   â”‚   â”œâ”€â”€ __init__.py           # Public API exports
â”‚   â”‚   â”œâ”€â”€ graph.py              # LangGraph workflow definition (76 LOC)
â”‚   â”‚   â”œâ”€â”€ nodes.py              # Agent node implementations (788 LOC)
â”‚   â”‚   â”œâ”€â”€ prompts/              # Modular LLM prompt templates (655 LOC)
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py       # Prompt exports
â”‚   â”‚   â”‚   â”œâ”€â”€ intent.py         # Intent classification & general responses (68 LOC)
â”‚   â”‚   â”‚   â”œâ”€â”€ sql.py            # SQL generation prompts (154 LOC)
â”‚   â”‚   â”‚   â”œâ”€â”€ visualization.py  # Chart & visualization prompts (87 LOC)
â”‚   â”‚   â”‚   â”œâ”€â”€ analysis.py       # Pyodide analysis prompts (63 LOC)
â”‚   â”‚   â”‚   â””â”€â”€ privacy.py        # PII masking & response prompts (245 LOC)
â”‚   â”‚   â”œâ”€â”€ helpers.py            # Reusable utilities (147 LOC)
â”‚   â”‚   â”œâ”€â”€ config.py             # LLM instances & constants (63 LOC)
â”‚   â”‚   â”œâ”€â”€ routing.py            # Conditional routing logic (106 LOC)
â”‚   â”‚   â”œâ”€â”€ feedbacks.py          # Error feedback messages (306 LOC)
â”‚   â”‚   â”œâ”€â”€ error_feedback.py     # Error feedback router (114 LOC)
â”‚   â”‚   â””â”€â”€ state.py              # State management schema (65 LOC)
â”‚   â”‚
â”‚   â”œâ”€â”€ data_pipeline/            # ETL and data preparation (1,606 LOC)
â”‚   â”‚   â”œâ”€â”€ __init__.py           # Public API exports
â”‚   â”‚   â”œâ”€â”€ profiler.py           # CSV data profiler (91 LOC)
â”‚   â”‚   â”œâ”€â”€ relationship_discovery.py  # Automatic FK detection (226 LOC)
â”‚   â”‚   â”œâ”€â”€ integrity_checker.py  # Data validation utilities (261 LOC)
â”‚   â”‚   â”œâ”€â”€ load_data.py          # CSV to DB ETL pipeline (167 LOC)
â”‚   â”‚   â”œâ”€â”€ transform_data.py     # Interactive data transformation (231 LOC)
â”‚   â”‚   â”œâ”€â”€ pii_discovery.py      # LLM-based PII column detection (189 LOC)
â”‚   â”‚   â”œâ”€â”€ generate_schema.py    # Schema + PII metadata generator (226 LOC)
â”‚   â”‚   â””â”€â”€ setup.py              # Automated pipeline orchestrator (190 LOC)
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                     # Shared utilities (588 LOC)
â”‚   â”‚   â”œâ”€â”€ __init__.py           # Public API exports
â”‚   â”‚   â”œâ”€â”€ console.py            # Unified CLI output formatting (72 LOC)
â”‚   â”‚   â”œâ”€â”€ db.py                 # Database connection management (84 LOC)
â”‚   â”‚   â”œâ”€â”€ logger.py             # Logging configuration (44 LOC)
â”‚   â”‚   â”œâ”€â”€ errors.py             # Custom exception classes (55 LOC)
â”‚   â”‚   â””â”€â”€ validation.py         # SQL validation & security (302 LOC)
â”‚   â”‚
â”‚   â”œâ”€â”€ config/                   # Configuration and metadata
â”‚   â”‚   â”œâ”€â”€ keys.json             # PK/FK metadata configuration
â”‚   â”‚   â”œâ”€â”€ schema_info.json      # Generated schema (used by LLM)
â”‚   â”‚   â”œâ”€â”€ schema_info.md        # Human-readable schema docs
â”‚   â”‚   â””â”€â”€ data_profile.json     # Data profiling statistics
â”‚   â”‚
â”‚   â”œâ”€â”€ setup.py                  # Automated pipeline orchestrator
â”‚   â”œâ”€â”€ reset_db.py               # Database + config reset utility
â”‚   â””â”€â”€ cli.py                    # CLI entry point
â”‚
â”œâ”€â”€ frontend/                     # Next.js 15 + React 19 frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/                  # Next.js App Router
â”‚   â”‚   â”œâ”€â”€ components/           # React components
â”‚   â”‚   â”‚   â”œâ”€â”€ plotly-chart.tsx  # Plotly visualization
â”‚   â”‚   â”‚   â””â”€â”€ python-runner.tsx # Pyodide runtime
â”‚   â”‚   â”œâ”€â”€ providers/            # Context providers & LangGraph client
â”‚   â”‚   â””â”€â”€ hooks/                # Custom React hooks
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ tailwind.config.js
â”‚
â”œâ”€â”€ data/                         # CSV data files (8 files)
â”‚   â”œâ”€â”€ sales_customers.csv
â”‚   â”œâ”€â”€ sales_franchises.csv
â”‚   â”œâ”€â”€ sales_suppliers.csv
â”‚   â”œâ”€â”€ sales_transactions.csv
â”‚   â”œâ”€â”€ media_customer_reviews.csv
â”‚   â”œâ”€â”€ media_gold_reviews_chunked.csv
â”‚   â”œâ”€â”€ media_campaigns.csv
â”‚   â””â”€â”€ missing_suppliers.csv
â”‚
â”œâ”€â”€ docs/                         # Documentation
â”‚   â”œâ”€â”€ ERROR-HANDLING.md         # Error handling & retry logic
â”‚   â”œâ”€â”€ DEVELOPMENT.md            # Development guide & contributing
â”‚   â””â”€â”€ ARCHITECTURE.md           # This file
â”‚
â”œâ”€â”€ tests/                        # Testing (672 LOC)
â”‚   â”œâ”€â”€ test_security.py          # Security validation tests (111 LOC)
â”‚   â”œâ”€â”€ agent/                    # Agent module tests
â”‚   â”‚   â”œâ”€â”€ test_routing.py       # Routing logic tests (143 LOC)
â”‚   â”‚   â”œâ”€â”€ test_helpers.py       # Helper utilities tests (185 LOC)
â”‚   â”‚   â””â”€â”€ test_config.py        # Configuration tests (100 LOC)
â”‚   â””â”€â”€ README.md                 # Testing documentation
â”‚
â”œâ”€â”€ docker-compose.yaml           # PostgreSQL + PgAdmin
â”œâ”€â”€ langgraph.json                # LangGraph configuration
â”œâ”€â”€ start.sh                      # One-command startup script
â”œâ”€â”€ environment.yml               # Conda environment (cross-platform)
â”œâ”€â”€ requirements.txt              # Python dependencies (pip)
â”œâ”€â”€ .env                          # Environment variables
â””â”€â”€ README.md                     # User-facing documentation
```

---

## High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Frontend (Browser)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ ChatGPT-style  â”‚  â”‚ Plotly Chart â”‚  â”‚ Pyodide (Python)   â”‚  â”‚
â”‚  â”‚ UI (React 19)  â”‚  â”‚ Renderer     â”‚  â”‚ Runtime (Pandas)   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚           â”‚                  â”‚                  â”‚                 â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                              â”‚                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚ HTTP/WebSocket (Streaming)
                               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              â–¼                                     â”‚
â”‚                    LangGraph Server (Port 2024)                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚              LangGraph State Machine (graph.py)             â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ â”‚
â”‚  â”‚  â”‚Intent     â”‚â†’ â”‚SQL Gen  â”‚â†’ â”‚Validationâ”‚â†’ â”‚Execution    â”‚â”‚ â”‚
â”‚  â”‚  â”‚Classifier â”‚  â”‚(LLM)    â”‚  â”‚(Security)â”‚  â”‚(PostgreSQL) â”‚â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚ â”‚
â”‚  â”‚                       â†“                           â†“          â”‚ â”‚
â”‚  â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚ â”‚
â”‚  â”‚                  â”‚Feedback â”‚â†â”€ Error? â”€â”€â”€â”€â”‚Check     â”‚     â”‚ â”‚
â”‚  â”‚                  â”‚(Retry)  â”‚               â”‚Result    â”‚     â”‚ â”‚
â”‚  â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                              â”‚                                     â”‚
â”‚                              â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                  LLM Provider (Cerebras)                     â”‚ â”‚
â”‚  â”‚              llama-3.3-70b (OpenAI-compatible API)          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     PostgreSQL 15 Database                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Customers  â”‚  â”‚ Orders     â”‚  â”‚ Products   â”‚  â”‚ Reviews   â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                      Foreign Key Constraints                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## LangGraph Workflow

### ğŸ¯ Quick Summary

**How Delivery Cadet answers your questions:**

1. **Question received** â†’ LLM generates SQL â†’ PostgreSQL executes â†’ Results returned
2. **SQL fails?** â†’ Error analysed, automatically retries up to 3 times (LLM receives feedback and fixes it)
3. **All 3 attempts fail?** â†’ Switches to simple SQL to fetch raw data â†’ Python (pandas) processes it in browser

> ğŸ’¡ **Core idea:** If complex SQL doesn't work â†’ Just fetch the data â†’ Let Python handle it in the user's browser

---

### ğŸ“– Real-World Examples

#### Scenario A: Success on First Try âœ…

```
User: "What were last month's sales?"
   â†“
LLM: SELECT SUM(amount) FROM orders WHERE created_at >= '2024-12-01'
   â†“
PostgreSQL executes â†’ Success!
   â†“
Result: "Last month's sales were $125,450."
```

#### Scenario B: Success After Retry ğŸ”„

```
User: "Show purchase count by customer"
   â†“
[Attempt 1] LLM: SELECT customer, COUNT(*) FROM it GROUP BY 1  âŒ
Error: "Unknown tables: {'it'}"
   â†“
Feedback: "Table name error. Available: customers, orders. No abbreviations."
   â†“
[Attempt 2] LLM: SELECT customer_name, COUNT(*) FROM orders GROUP BY 1  âœ…
PostgreSQL executes â†’ Success!
   â†“
Result: Bar chart showing purchases by customer
```

#### Scenario C: Pyodide Fallback ğŸ

```
User: "Analyse sales trends and correlations by date"
   â†“
[Attempts 1-3] Complex SQL with date functions fails 3 times âŒ
   â†“
System: "SQL's not cutting it... switching strategy!"
   â†“
[Attempt 4] Simple SQL: SELECT date, amount FROM orders  âœ…
   â†“
Python executes in browser:
  df = pandas.DataFrame(data)
  df['date'] = pandas.to_datetime(df['date'])
  correlation = df['amount'].corr(df['date'])
   â†“
Result: "The correlation coefficient between sales and date is 0.73."
```

---

### ğŸ¬ Step-by-Step Guide

#### Step 1: Receive Question ğŸ“
```
[read_question]
Extract question from user message
   â†“
state.user_question = "What were last month's sales?"
```

#### Step 2: Classify Intent ğŸ¤”
```
[intent_classification]
LLM analyses question (temperature: 0.0 = consistent)
   â†“
Decision: "sales" = data query required
   â†“
state.intent = "sql"  (or "general")
```
- **"sql"** â†’ Database query needed
- **"general"** â†’ Casual chat (e.g., "hello", "thanks")

#### Step 3: Decide Analysis Method ğŸ”
```
[pyodide_request_classification]
Check for advanced analysis keywords:
  - "correlation", "regression", "standard deviation"
  - "statistics", "distribution", "outliers"
   â†“
None found â†’ SQL alone is sufficient
Found â†’ Python needed (simple SQL + Pandas)
   â†“
state.needs_pyodide = True/False
```

#### Step 4: Generate & Execute SQL ğŸ’»
```
[generate_SQL]
1. Load schema: schema_info.json (cached)
2. Check needs_pyodide flag:
   - False â†’ Complex SQL (JOIN, GROUP BY, aggregations)
   - True â†’ Simple SQL (SELECT columns only)
3. Ask LLM to generate SQL (temperature: 0.1 = accuracy)
4. Security validation: Check for DROP/DELETE/--
   â†“
state.sql_query = "SELECT ..."
   â†“
[execute_SQL]
PostgreSQL executes
   â†“
state.query_result = '[{"col": "val"}]'  (or "Error: ...")
```

**ğŸ”„ If it fails?**  
â†’ See [Retry Mechanism](#retry-mechanism-automatic-recovery)

#### Step 5: Determine if Visualisation Needed ğŸ“Š
```
[visualisation_request_classification]
Analyse question:
  - Contains "chart", "graph", "visualise" keywords?
  - Is data suitable for visualisation?
   â†“
Yes â†’ LLM generates chart title + creates Plotly chart
No â†’ Text response only
   â†“
state.plotly_data = '{"type": "bar", ...}'  (or None)
```

#### Step 6: Python Analysis (If Needed) ğŸ
```
[generate_pyodide_analysis]  (only when needs_pyodide=True)
LLM generates pandas code:
  import pandas as pd
  df = pd.DataFrame(data)
  result = df.groupby('category')['amount'].mean()
   â†“
Pyodide executes in browser
   â†“
Result stored as ToolMessage
```

#### Step 7: Generate Final Response âœï¸
```
[generate_response]
LLM synthesises (temperature: 0.7 = natural):
  - SQL results
  - Chart (if present)
  - Python analysis (if present)
   â†“
Converts to natural language
   â†“
Streams to user
```

---

### ğŸ”„ Retry Mechanism (Automatic Recovery)

#### ğŸ¯ Core Rules

| Situation | Action | Counter | Next Step |
|-----------|--------|---------|-----------|
| SQL succeeds âœ… | Proceed | - | Visualisation check |
| SQL fails (1-2 times) âŒ | Analyse error â†’ Retry | +1 | Regenerate SQL |
| SQL fails (3 times) ğŸ’¥ | Enable Pyodide fallback | Reset â†’ 0 | Simple SQL |
| Pyodide also fails ğŸš« | Give up | - | Error message |

#### ğŸ“– Detailed Flow

```
execute_SQL
   â†“
Check result (routing.py: decide_sql_retry_route)
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Success âœ… â”‚  Failed âŒ   â”‚  None âš ï¸    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚             â”‚             â”‚
       â–¼             â–¼             â–¼
return "success"  is_error_result?  return "retry"
       â”‚             â”‚
       â”‚        â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
       â”‚        â”‚  True   â”‚
       â”‚        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
       â”‚             â”‚
       â”‚        retry_count < 3?
       â”‚             â”‚
       â”‚        â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
       â”‚    Yes â”‚     No  â”‚
       â”‚        â–¼         â–¼
       â”‚    "retry"   fallback_attempted?
       â”‚        â”‚         â”‚
       â”‚        â”‚    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
       â”‚        â”‚  No â”‚    Yes  â”‚
       â”‚        â”‚    â–¼         â–¼
       â”‚        â”‚"fallback" "success"
       â”‚        â”‚    â”‚      (give up)
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
        Route to next node
```

#### ğŸ’¡ Actual Code Implementation

**State Variables (state.py):**
- `sql_retry_count`: Number of SQL failures (0-3)
- `pyodide_fallback_attempted`: Whether fallback has been tried (True/False)
- `query_result`: Execution result or "Error: ..." string

**Routing Logic (routing.py: decide_sql_retry_route):**
```python
# 1. Result is None â†’ retry
if result is None:
    return "retry"

# 2. Check for errors
if is_error_result(result):  # Checks if starts with "Error:"
    retry_count = state.get('sql_retry_count', 0) or 0
    
    # Less than 3 attempts â†’ retry
    if retry_count < max_retries:
        return "retry"  # sql_retry_count++ handled in execute_SQL
    
    # 3 or more attempts â†’ check fallback status
    fallback_attempted = state.get('pyodide_fallback_attempted', False)
    if not fallback_attempted:
        return "fallback"  # Switch to Pyodide mode
    else:
        return "success"  # Give up, pass error message

# 3. Success
return "success"
```

**Enable Fallback (nodes.py: enable_pyodide_fallback):**
```python
state['needs_pyodide'] = True  # Simple SQL mode
state['sql_retry_count'] = 0   # Reset counter
state['query_result'] = None   # Clear error
state['pyodide_fallback_attempted'] = True  # Prevent infinite loop
```

---

### ğŸ”§ Technical Details

<details>
<summary>ğŸ“ Complete State Machine Diagram (Click to expand)</summary>

### State Machine Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    START    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ read_question   â”‚  Extract user question from messages
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ intent_classificationâ”‚  Classify as "sql" or "general"
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  (Temperature: 0.0 - deterministic)
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
    â”‚             â”‚
    â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚pyodide_request_              â”‚  â”‚generate_general_responseâ”‚
â”‚      classification          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                              â”‚              â”‚ (Temperature: 0.7)
â”‚ Check for analysis keywords  â”‚              â–¼
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”Œâ”€â”€â”€â”€â”€â”
            â”‚                              â”‚ END â”‚
      â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                       â””â”€â”€â”€â”€â”€â”˜
      â”‚            â”‚
      â–¼            â–¼
 [needs_pyodide=True]  [needs_pyodide=False]
      â”‚            â”‚
      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚generate_  â”‚  Uses simple SQL (needs_pyodide=True)
      â”‚   SQL     â”‚  or complex SQL (needs_pyodide=False)
      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  (Temperature: 0.1 - accurate)
            â”‚
            â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚execute_SQLâ”‚
      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
            â”‚
      â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â” (check_query_validation)
      â”‚            â”‚
  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”    â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
  â”‚[retry]â”‚    â”‚[fallback] â”‚  sql_retry_count >= 3
  â”‚       â”‚    â”‚           â”‚  AND !pyodide_fallback_attempted
  â””â”€â”€â”€â”¬â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
      â”‚              â”‚
      â”‚              â–¼
      â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚        â”‚enable_pyodide_      â”‚  Set needs_pyodide=True
      â”‚        â”‚    fallback         â”‚  Reset sql_retry_count=0
      â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  Clear query errors
      â”‚                   â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼ (both retry paths lead back to generate_SQL)

      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚     [success]                â”‚  No errors OR max retries exceeded
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  with fallback already attempted
                 â”‚
                 â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚visualisation_request_        â”‚
      â”‚      classification          â”‚  Determine if chart is needed
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  (Temperature: 0.0 - strict)
                  â”‚
            â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â” (check_pyodide_classification)
            â”‚            â”‚
            â–¼            â–¼
   [needs_pyodide]   [skip]
            â”‚            â”‚
            â–¼            â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
   â”‚generate_pyodide_â”‚  â”‚  Generate pandas analysis code
   â”‚    analysis     â”‚  â”‚  (Injects CSV data)
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
            â”‚            â”‚
            â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                 â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚generate_      â”‚
            â”‚  response     â”‚  Format natural language answer
            â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  (Temperature: 0.7 - conversational)
                    â”‚
                    â–¼
                 â”Œâ”€â”€â”€â”€â”€â”
                 â”‚ END â”‚
                 â””â”€â”€â”€â”€â”€â”˜
```

</details>

---

## Key Components

### 1. Intent Classification

- **Node:** `intent_classification`
- **LLM Temperature:** 0.0 (deterministic)
- **Purpose:** Routes between SQL generation and general conversation
- **Output:** `"sql"` or `"general"`

### 2. SQL Generation

- **Node:** `generate_SQL`
- **LLM Temperature:** 0.1 (accurate, low variance)
- **Prompt Selection:** Conditional based on `needs_pyodide` flag
  - **Simple SQL Prompt** (pyodide=True): Fetches raw data for Python analysis
    - No aggregations (AVG, SUM, COUNT)
    - No window functions (PARTITION BY, RANK)
    - No date functions (EXTRACT, TO_DATE, DATE_TRUNC)
    - Just SELECT columns AS-IS for Pandas processing
  - **Complex SQL Prompt** (pyodide=False): Full analytical queries
    - Aggregations, joins, subqueries allowed
    - Database performs all computation
- **Process:**
  1. Load schema from `schema_info.json` (cached)
  2. Check `needs_pyodide` flag from earlier classification
  3. Select appropriate prompt (simple vs complex)
  4. If retry: Add error-specific feedback from `feedbacks.py`
  5. Call LLM to generate SQL
  6. Parse XML response: `<reasoning>` + `<sql>`
  7. Validate SQL for security and correctness

### 3. SQL Validation (validation.py)

- **Purpose:** Prevent SQL injection and ensure query correctness
- **Checks:**
  1. Forbidden keywords (DROP, DELETE, UPDATE, etc.)
  2. Multiple statements (semicolon check)
  3. Comments (-- or /\* \*/)
  4. Unknown table names (with CTE/alias filtering)
- **Table Extraction Logic:**
  - Uses `sqlparse` to parse SQL into tokens
  - Skips `Function` and `Parenthesis` tokens to avoid false positives
  - **Implementation:** Function/Parenthesis tokens are skipped entirely without recursion to avoid extracting function arguments as table names
  - Only statement-level FROM/JOIN clauses are processed for table extraction
- **On Error:** Raises `SQLGenerationError` with detailed debug logs

### 4. Query Execution

- **Node:** `execute_SQL`
- **Process:**
  1. Skip execution if validation error already in `query_result`
  2. Execute SQL via SQLAlchemy
  3. Apply PII masking (deterministic, Python-based)
  4. Return JSON result or error message
- **Retry Limit:** 3 attempts (tracked via `sql_retry_count` in state)
- **Error Handling:**
  - Validation errors: Skips execution, passes error through
  - Database errors: Increments `sql_retry_count`, stores error in `query_result`

### 5. Visualization Request Classification

- **Node:** `visualisation_request_classification`
- **LLM Temperature:** 0.0 (strict keyword detection)
- **Keywords:** "chart", "graph", "plot", "visualize", "visualization", "draw"
- **Default:** `"no"` (prevents over-generation)
- **Chart Types:**
  - bar (comparison/ranking)
  - line (time series trends)
  - pie (proportions/breakdown)
  - scatter (correlation between two numeric variables)
  - area (time series with cumulative/fill emphasis)

### 6. Pyodide Request Classification

- **Node:** `pyodide_request_classification`
- **Execution Order:** **BEFORE** SQL generation (prevents complex SQL when simple data fetch is needed)
- **Method:** Keyword-based detection
- **Keywords:**
  - `correlation`, `statistical analysis`, `statistics`
  - `standard deviation`, `variance`, `mean`
  - `distribution`, `skewness`, `kurtosis`
  - `outlier`, `outliers`, `percentile`, `quartile`
  - `time series`, `trend`, `seasonality`
  - `describe`, `summary`
- **Output:** `needs_pyodide` boolean (stored in state for later use)
- **Purpose:** Triggers simple SQL prompt to fetch raw data instead of performing analysis in database
- **Future Enhancement:** LLM-based classification with multilingual support

### 7. Chart Generation

- **Technology:** Plotly.js + react-plotly.js
- **Process:**
  1. Determine chart type from user question
  2. Extract x/y axes from SQL result columns
  3. Generate dynamic title from user question (60 char limit)
  4. Apply PII masking to data
  5. Return Plotly JSON spec to frontend

### 8. In-Browser Python Execution

- **Technology:** Pyodide (WebAssembly Python) + react-py
- **Libraries:** pandas (data manipulation)
- **Process:**
  1. LLM generates pandas analysis code
  2. Data injected as CSV format (not JSON) for efficiency
  3. Frontend loads Pyodide runtime
  4. Execute code in browser sandbox
  5. Display results in UI
- **Security:** No server-side code execution
- **Data Format:** CSV string injected directly into Python code to prevent JSON parsing overhead

### 9. Pyodide Fallback Mechanism

- **Node:** `enable_pyodide_fallback`
- **Trigger:** After 3 consecutive SQL generation/execution failures
- **Purpose:** Recover from complex SQL errors by simplifying the query strategy
- **Process:**
  1. Set `needs_pyodide=True` to enable simple SQL mode
  2. Reset `sql_retry_count=0` for fresh retry attempts
  3. Clear `query_result` and `sql_query` error states
  4. Set `pyodide_fallback_attempted=True` to prevent infinite loops
  5. Route back to `generate_SQL` with simplified prompt
- **Fallback Strategy:**
  - **Before fallback:** Complex SQL with aggregations, window functions, date operations
  - **After fallback:** Simple SELECT to fetch raw data, analysis delegated to Pyodide (pandas)
- **Guard Mechanism:** If fallback also fails after 3 attempts, routes to `generate_response` with error message

### 10. Response Generation

- **Node:** `generate_response`
- **LLM Temperature:** 0.7 (natural, varied)
- **Output Format:** `<answer>` + `<insight>` (XML tags)
- **PII:** Already masked in data
- **Streaming:** Real-time response streaming to frontend

### 11. Error Feedback System

- **Architecture:** Two-layer system for targeted error correction
  - **feedbacks.py (306 LOC):** Message templates for each error type
  - **error_feedback.py (114 LOC):** Router that analyzes errors and selects appropriate feedback
- **Purpose:** Provide LLM-specific hints for error correction during retry attempts
- **Process:**
  1. `nodes.py` calls `get_sql_error_feedback(error_message, allowed_tables)`
  2. `error_feedback.py` analyzes error message (regex matching)
  3. Routes to appropriate feedback generator in `feedbacks.py`
  4. Returns targeted hint string to append to SQL generation prompt
- **Feedback Types (feedbacks.py):**
  - `get_unknown_tables_feedback()` - Invalid table names (with alias detection)
  - `get_multiple_statements_feedback()` - Semicolon usage (suggest CTE)
  - `get_sql_comments_feedback()` - Comment removal
  - `get_forbidden_keyword_feedback()` - Dangerous keywords (CREATE, DROP, etc.)
  - `get_column_not_found_feedback()` - Case sensitivity and quoting rules
  - `get_division_by_zero_feedback()` - NULLIF usage
  - `get_datetime_format_feedback()` - Direct casting instead of TO_TIMESTAMP
  - `get_alias_reference_feedback()` - Alias in same SELECT clause (suggest CTE)
  - `get_parsing_error_feedback()` - Generic syntax errors (catch-all)
- **Example:** "Your previous attempt used invalid table 'it'. Use ONLY: customers, orders, products..."
- **Separation of Concerns:**
  - **feedbacks.py** stores human-readable messages (easy to modify/translate)
  - **error_feedback.py** handles error analysis logic (pattern matching)

---

## Data Flow

### User Question â†’ SQL Result

```
1. User Input
   "Show me top 5 customers by total spending"

2. Intent Classification (LLM)
   â†’ intent: "sql"

3. SQL Generation (LLM + Schema)
   schema_info.json â†’ Prompt
   â†’ SQL: SELECT c."name", SUM(o."amount") as total FROM customers c JOIN orders o...

4. SQL Validation (validation.py)
   âœ“ No dangerous keywords
   âœ“ Single statement
   âœ“ No comments
   âœ“ Tables exist: customers, orders

5. Query Execution (PostgreSQL)
   â†’ Result: [{"name": "Person #1", "total": 15000}, ...]
   â†’ PII masked: "John Smith" â†’ "Person #1"

6. Visualization Check (LLM)
   â†’ visualise: "no" (no chart keywords in question)

7. Pyodide Check (Keyword)
   â†’ needs_pyodide: false

8. Response Generation (LLM)
   â†’ Answer: "The top 5 customers by total spending are..."
   â†’ Insight: "Person #1 accounts for 30% of total revenue..."

9. Frontend Display
   â†’ Streaming text response to user
```

---

## Dataset-Agnostic Design

### Core Principle

**All table/column information is loaded from `schema_info.json` at runtime, not hardcoded in prompts.**

### Implementation

1. **Schema Generation** (generate_schema.py)

   - Reads database metadata
   - Detects PII columns via LLM
   - Outputs `schema_info.json`

2. **Runtime Loading** (nodes.py:95-120)

   ```python
   def load_schema_info() -> str:
       global _SCHEMA_CACHE
       if _SCHEMA_CACHE is None:
           with open(SCHEMA_JSON_PATH, 'r') as f:
               schema_data = json.load(f)
           _SCHEMA_CACHE = format_schema_for_prompt(schema_data)
       return _SCHEMA_CACHE
   ```

3. **Prompt Injection** (prompts.py:85-175)

   ```python
   def get_sql_generation_prompt(schema_info: str, user_question: str) -> str:
       return f"""
       <database_schema>
       {schema_info}
       </database_schema>

       <user_question>
       {user_question}
       </user_question>
       """
   ```

### Benefits

- âœ… Swap datasets by replacing CSVs and re-running pipeline
- âœ… No code changes required for new schemas
- âœ… Scalable to different domains (retail, healthcare, finance, etc.)

---

## Module Responsibilities

### agent/ - LangGraph Workflow

- **graph.py:** StateGraph definition, conditional edges, retry logic, Pyodide fallback routing
- **nodes.py:** Node implementations, LLM calls, error handling, PII masking
- **prompts/:** Modular LLM prompt templates organized by function
  - **intent.py:** Intent classification & general conversation responses
  - **sql.py:** SQL generation prompts (complex & simple modes for Pyodide)
  - **visualization.py:** Chart request detection & title generation
  - **analysis.py:** Pyodide analysis code generation prompts
  - **privacy.py:** PII masking & natural language response formatting
- **helpers.py:** Reusable utilities (schema caching, DB engine pooling, PII masking)
- **config.py:** LLM instances with task-specific temperatures & workflow constants
- **routing.py:** Conditional routing logic (RouteDecider class with static methods)
- **feedbacks.py:** Error feedback messages - actual feedback strings for each error type
- **error_feedback.py:** Error feedback router - analyzes errors and routes to appropriate feedback
- **state.py:** TypedDict schema for state management (includes fallback flags and retry counters)

### core/ - Shared Utilities

- **validation.py:** SQL security checks, table name validation
- **db.py:** Database connection pooling (singleton pattern)
- **logger.py:** Structured logging configuration
- **errors.py:** Custom exception hierarchy
- **console.py:** CLI output formatting

### data_pipeline/ - ETL

- **profiler.py:** Analyze CSV structure and statistics
- **relationship_discovery.py:** Suggest FK relationships (interactive)
- **load_data.py:** CSV â†’ PostgreSQL with constraints
- **integrity_checker.py:** Validate PK/FK integrity, detect offsets
- **transform_data.py:** Interactive SQL console for data fixes
- **pii_discovery.py:** LLM-based PII column detection
- **generate_schema.py:** Create schema metadata + PII report
- **setup.py:** Automated pipeline orchestration

---

## Performance Optimizations

### 1. Caching

- **Schema:** Loaded once, cached globally (`_SCHEMA_CACHE`)
- **Database Engine:** Connection pool reused (`_DB_ENGINE`)
- **Frontend:** Chart stability via revision prop and explicit sizing

### 2. Temperature Tuning

- **Intent (0.0):** Deterministic routing
- **SQL (0.1):** Accurate, minimal hallucination
- **Visualization (0.0):** Strict keyword matching
- **Response (0.7):** Natural, varied language

### 3. Streaming

- **LangGraph Server:** Real-time response streaming
- **Frontend:** Progressive UI updates

### 4. Connection Pooling

- **SQLAlchemy:** pool_size=5, max_overflow=10
- **Reuses connections** across requests

---

## Security Layers

### 1. SQL Injection Prevention (validation.py)

- Forbidden keyword blocking
- Multiple statement prevention
- Comment removal
- Table name whitelist validation

### 2. PII Masking (nodes.py:128-194)

- LLM-based detection during schema generation
- Deterministic masking at query execution
- Person names â†’ "Person #N" (sequential)
- Organization names preserved

### 3. Read-Only Access

- Only SELECT queries allowed
- No write operations (INSERT, UPDATE, DELETE)
- No schema modifications (CREATE, ALTER, DROP)

### 4. Rate Limiting

- Max retry limit: 3 attempts
- Prevents infinite loops

---

## Technology Choices

### Why LangGraph?

- **State Management:** Built-in state persistence
- **Conditional Routing:** Easy error handling with conditional edges
- **Streaming:** Native streaming support
- **Debugging:** LangSmith integration for trace visualization

### Why Cerebras (llama-3.3-70b)?

- **Performance:** Fast inference (previously Groq)
- **OpenAI-compatible API:** Easy integration
- **Cost-effective:** Competitive pricing

### Why PostgreSQL?

- **Relational:** Strong FK constraint support
- **JSON Support:** Native JSON column types
- **Mature:** Well-documented, stable

### Why Next.js 15?

- **App Router:** Server components, streaming
- **React 19:** Latest features (concurrent rendering)
- **TypeScript:** Type safety

---

## Related Documentation

- [Error Handling Guide](ERROR-HANDLING.md) - SQL validation, retry logic, debugging
- [Development Guide](DEVELOPMENT.md) - Contributing and extending
- README.md - User setup and usage

---

**Last Updated:** 2026-01-11
