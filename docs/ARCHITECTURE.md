# System Architecture

This document provides a comprehensive overview of Delivery Cadet's system design, component structure, and data flow.

## Table of Contents
- [Project Structure](#project-structure)
- [High-Level Architecture](#high-level-architecture)
- [LangGraph Workflow](#langgraph-workflow)
- [Key Components](#key-components)
- [Data Flow](#data-flow)
- [Dataset-Agnostic Design](#dataset-agnostic-design)
- [Module Responsibilities](#module-responsibilities)

---

## Project Structure

```
cadet/
â”œâ”€â”€ src/                          # Python backend source code (3,872 LOC)
â”‚   â”œâ”€â”€ agent/                    # LangGraph agent workflow (1,503 LOC)
â”‚   â”‚   â”œâ”€â”€ __init__.py           # Public API exports
â”‚   â”‚   â”œâ”€â”€ graph.py              # LangGraph workflow definition (95 LOC)
â”‚   â”‚   â”œâ”€â”€ nodes.py              # Agent node implementations (791 LOC)
â”‚   â”‚   â”œâ”€â”€ prompts.py            # LLM prompt templates (517 LOC)
â”‚   â”‚   â”œâ”€â”€ feedbacks.py          # Error feedback messages (268 LOC) ğŸ†•
â”‚   â”‚   â””â”€â”€ state.py              # State management schema (60 LOC)
â”‚   â”‚
â”‚   â”œâ”€â”€ data_pipeline/            # ETL and data preparation (1,606 LOC)
â”‚   â”‚   â”œâ”€â”€ __init__.py           # Public API exports
â”‚   â”‚   â”œâ”€â”€ profiler.py           # CSV data profiler (91 LOC)
â”‚   â”‚   â”œâ”€â”€ relationship_discovery.py  # Automatic FK detection (226 LOC)
â”‚   â”‚   â”œâ”€â”€ integrity_checker.py  # Data validation utilities (261 LOC)
â”‚   â”‚   â”œâ”€â”€ load_data.py          # CSV to DB ETL pipeline (167 LOC)
â”‚   â”‚   â”œâ”€â”€ transform_data.py     # Interactive data transformation (231 LOC)
â”‚   â”‚   â”œâ”€â”€ pii_discovery.py      # LLM-based PII column detection (189 LOC)
â”‚   â”‚   â”œâ”€â”€ generate_schema.py    # Schema + PII metadata generator (226 LOC)
â”‚   â”‚   â””â”€â”€ setup.py              # Automated pipeline orchestrator (190 LOC)
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                     # Shared utilities (588 LOC)
â”‚   â”‚   â”œâ”€â”€ __init__.py           # Public API exports
â”‚   â”‚   â”œâ”€â”€ console.py            # Unified CLI output formatting (72 LOC)
â”‚   â”‚   â”œâ”€â”€ db.py                 # Database connection management (84 LOC)
â”‚   â”‚   â”œâ”€â”€ logger.py             # Logging configuration (44 LOC)
â”‚   â”‚   â”œâ”€â”€ errors.py             # Custom exception classes (55 LOC)
â”‚   â”‚   â””â”€â”€ validation.py         # SQL validation & security (302 LOC)
â”‚   â”‚
â”‚   â”œâ”€â”€ config/                   # Configuration and metadata
â”‚   â”‚   â”œâ”€â”€ keys.json             # PK/FK metadata configuration
â”‚   â”‚   â”œâ”€â”€ schema_info.json      # Generated schema (used by LLM)
â”‚   â”‚   â”œâ”€â”€ schema_info.md        # Human-readable schema docs
â”‚   â”‚   â””â”€â”€ data_profile.json     # Data profiling statistics
â”‚   â”‚
â”‚   â”œâ”€â”€ setup.py                  # Automated pipeline orchestrator
â”‚   â”œâ”€â”€ reset_db.py               # Database + config reset utility
â”‚   â””â”€â”€ cli.py                    # CLI entry point
â”‚
â”œâ”€â”€ frontend/                     # Next.js 15 + React 19 frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/                  # Next.js App Router
â”‚   â”‚   â”œâ”€â”€ components/           # React components
â”‚   â”‚   â”‚   â”œâ”€â”€ plotly-chart.tsx  # Plotly visualization
â”‚   â”‚   â”‚   â””â”€â”€ python-runner.tsx # Pyodide runtime
â”‚   â”‚   â”œâ”€â”€ providers/            # Context providers & LangGraph client
â”‚   â”‚   â””â”€â”€ hooks/                # Custom React hooks
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ tailwind.config.js
â”‚
â”œâ”€â”€ data/                         # CSV data files (8 files)
â”‚   â”œâ”€â”€ sales_customers.csv
â”‚   â”œâ”€â”€ sales_franchises.csv
â”‚   â”œâ”€â”€ sales_suppliers.csv
â”‚   â”œâ”€â”€ sales_transactions.csv
â”‚   â”œâ”€â”€ media_customer_reviews.csv
â”‚   â”œâ”€â”€ media_gold_reviews_chunked.csv
â”‚   â”œâ”€â”€ media_campaigns.csv
â”‚   â””â”€â”€ missing_suppliers.csv
â”‚
â”œâ”€â”€ docs/                         # Documentation
â”‚   â”œâ”€â”€ ERROR-HANDLING.md         # Error handling & retry logic
â”‚   â”œâ”€â”€ ARCHITECTURE.md           # This file
â”‚   â””â”€â”€ sql.md                    # SQL patterns & examples
â”‚
â”œâ”€â”€ tests/                        # Testing
â”‚   â””â”€â”€ test_security.py          # Security validation tests
â”‚
â”œâ”€â”€ docker-compose.yaml           # PostgreSQL + PgAdmin
â”œâ”€â”€ langgraph.json                # LangGraph configuration
â”œâ”€â”€ start.sh                      # One-command startup script
â”œâ”€â”€ environment.yml               # Conda environment (cross-platform)
â”œâ”€â”€ requirements.txt              # Python dependencies (pip)
â”œâ”€â”€ .env                          # Environment variables
â””â”€â”€ README.md                     # User-facing documentation
```

---

## High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Frontend (Browser)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ ChatGPT-style  â”‚  â”‚ Plotly Chart â”‚  â”‚ Pyodide (Python)   â”‚  â”‚
â”‚  â”‚ UI (React 19)  â”‚  â”‚ Renderer     â”‚  â”‚ Runtime (Pandas)   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚           â”‚                  â”‚                  â”‚                 â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                              â”‚                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚ HTTP/WebSocket (Streaming)
                               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              â–¼                                     â”‚
â”‚                    LangGraph Server (Port 2024)                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚              LangGraph State Machine (graph.py)             â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ â”‚
â”‚  â”‚  â”‚Intent     â”‚â†’ â”‚SQL Gen  â”‚â†’ â”‚Validationâ”‚â†’ â”‚Execution    â”‚â”‚ â”‚
â”‚  â”‚  â”‚Classifier â”‚  â”‚(LLM)    â”‚  â”‚(Security)â”‚  â”‚(PostgreSQL) â”‚â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚ â”‚
â”‚  â”‚                       â†“                           â†“          â”‚ â”‚
â”‚  â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚ â”‚
â”‚  â”‚                  â”‚Feedback â”‚â†â”€ Error? â”€â”€â”€â”€â”‚Check     â”‚     â”‚ â”‚
â”‚  â”‚                  â”‚(Retry)  â”‚               â”‚Result    â”‚     â”‚ â”‚
â”‚  â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                              â”‚                                     â”‚
â”‚                              â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                  LLM Provider (Cerebras)                     â”‚ â”‚
â”‚  â”‚              llama-3.3-70b (OpenAI-compatible API)          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     PostgreSQL 15 Database                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Customers  â”‚  â”‚ Orders     â”‚  â”‚ Products   â”‚  â”‚ Reviews   â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                      Foreign Key Constraints                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## LangGraph Workflow

### State Machine Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    START    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ read_question   â”‚  Extract user question from messages
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ intent_classificationâ”‚  Classify as "sql" or "general"
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  (Temperature: 0.0 - deterministic)
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
    â”‚             â”‚
    â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚pyodide_request_              â”‚  â”‚generate_general_responseâ”‚
â”‚      classification          â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                              â”‚              â”‚ (Temperature: 0.7)
â”‚ Check for analysis keywords  â”‚              â–¼
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”Œâ”€â”€â”€â”€â”€â”
            â”‚                              â”‚ END â”‚
      â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                       â””â”€â”€â”€â”€â”€â”˜
      â”‚            â”‚
      â–¼            â–¼
 [needs_     [skip_
  pyodide]    pyodide]
      â”‚            â”‚
      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
           â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚generate_  â”‚  Uses simple SQL (pyodide=True)
      â”‚   SQL     â”‚  or complex SQL (pyodide=False)
      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  (Temperature: 0.1 - accurate)
            â”‚
            â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚execute_SQLâ”‚
      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
            â”‚
        â”Œâ”€â”€â”€â”´â”€â”€â”€â”
        â”‚ Error?â”‚  (check_query_validation)
        â””â”€â”€â”€â”¬â”€â”€â”€â”˜
            â”‚
        â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
        â”‚           â”‚
        â–¼           â–¼
    [retry]     [success]
        â”‚           â”‚
        â”‚           â–¼
        â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚     â”‚visualisation_request_        â”‚
        â”‚     â”‚      classification          â”‚  Keyword-based chart detection
        â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  (Temperature: 0.0 - strict)
        â”‚                 â”‚
        â”‚           â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
        â”‚           â”‚            â”‚
        â”‚           â–¼            â–¼
        â”‚     [needs_viz]    [skip_viz]
        â”‚           â”‚            â”‚
        â”‚           â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
        â”‚                 â”‚
        â”‚                 â–¼
        â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚     â”‚Check needs_pyodide from      â”‚
        â”‚     â”‚     earlier classification   â”‚
        â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                 â”‚
        â”‚           â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
        â”‚           â”‚            â”‚
        â”‚           â–¼            â–¼
        â”‚    [needs_pyodide] [skip]
        â”‚           â”‚            â”‚
        â”‚           â–¼            â”‚
        â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
        â”‚  â”‚generate_pyodide_â”‚  â”‚  Generate pandas analysis code
        â”‚  â”‚    analysis     â”‚  â”‚
        â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
        â”‚           â”‚            â”‚
        â”‚           â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                â–¼
        â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚           â”‚generate_      â”‚
        â”‚           â”‚  response     â”‚  Format natural language answer
        â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  (Temperature: 0.7 - conversational)
        â”‚                   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                            â–¼
                         â”Œâ”€â”€â”€â”€â”€â”
                         â”‚ END â”‚
                         â””â”€â”€â”€â”€â”€â”˜
```

### Retry Mechanism (Updated 2026-01-09)

**Before:** Validation errors caused workflow termination without retry.

**After:** Validation errors are stored in `query_result` and trigger the retry loop.

```
generate_SQL
    â†“
    LLM generates SQL
    â†“
validate_sql_query() (validation.py)
    â†“
â”Œâ”€ PASS â†’ execute_SQL â†’ Success
â”‚
â””â”€ FAIL â†’ Error stored in query_result + messages
              â†“
          execute_SQL (skips execution if error present)
              â†“
          check_query_validation (detects error in query_result)
              â†“
          â”Œâ”€ retry_count < 3 â†’ generate_SQL (with error feedback)
          â”‚
          â””â”€ retry_count >= 3 â†’ Return "Max retries exceeded" error
```

**Key Improvement:** `messages` array is updated with error to track retry count correctly, preventing infinite retry loops.

---

## Key Components

### 1. Intent Classification
- **Node:** `intent_classification`
- **LLM Temperature:** 0.0 (deterministic)
- **Purpose:** Routes between SQL generation and general conversation
- **Output:** `"sql"` or `"general"`

### 2. SQL Generation (Updated 2026-01-10)
- **Node:** `generate_SQL`
- **LLM Temperature:** 0.1 (accurate, low variance)
- **Prompt Selection:** Conditional based on `needs_pyodide` flag
  - **Simple SQL Prompt** (pyodide=True): Fetches raw data for Python analysis
    - No aggregations (AVG, SUM, COUNT)
    - No window functions (PARTITION BY, RANK)
    - No date functions (EXTRACT, TO_DATE, DATE_TRUNC)
    - Just SELECT columns AS-IS for Pandas processing
  - **Complex SQL Prompt** (pyodide=False): Full analytical queries
    - Aggregations, joins, subqueries allowed
    - Database performs all computation
- **Process:**
  1. Load schema from `schema_info.json` (cached)
  2. Check `needs_pyodide` flag from earlier classification
  3. Select appropriate prompt (simple vs complex)
  4. If retry: Add error-specific feedback from `feedbacks.py`
  5. Call LLM to generate SQL
  6. Parse XML response: `<reasoning>` + `<sql>`
  7. Validate SQL for security and correctness

### 3. SQL Validation (validation.py - Updated 2026-01-10)
- **Purpose:** Prevent SQL injection and ensure query correctness
- **Checks:**
  1. Forbidden keywords (DROP, DELETE, UPDATE, etc.)
  2. Multiple statements (semicolon check)
  3. Comments (-- or /* */)
  4. Unknown table names (with CTE/alias filtering)
- **Table Extraction Logic:**
  - Uses `sqlparse` to parse SQL into tokens
  - Skips `Function` and `Parenthesis` tokens to avoid false positives
  - **Bug Fix (2026-01-10):** Previously, `EXTRACT(DOW FROM "dateTime")` was incorrectly extracting "datetime" as a table name due to recursive processing of function tokens
  - **Solution:** Function/Parenthesis tokens are now skipped entirely without recursion
  - Only statement-level FROM/JOIN clauses are processed for table extraction
- **On Error:** Raises `SQLGenerationError` with detailed debug logs

### 4. Query Execution
- **Node:** `execute_SQL`
- **Process:**
  1. Skip execution if validation error already in `query_result`
  2. Execute SQL via SQLAlchemy
  3. Apply PII masking (deterministic, Python-based)
  4. Return JSON result or error message
- **Retry Limit:** 3 attempts (tracked via `messages` array)

### 5. Visualization Request Classification
- **Node:** `visualisation_request_classification`
- **LLM Temperature:** 0.0 (strict keyword detection)
- **Keywords:** "chart", "graph", "plot", "visualize", "visualization", "draw"
- **Default:** `"no"` (prevents over-generation)
- **Chart Types:** bar (comparison), line (time series), pie (proportions)

### 6. Pyodide Request Classification (Updated 2026-01-10)
- **Node:** `pyodide_request_classification`
- **Execution Order:** **BEFORE** SQL generation (prevents complex SQL when simple data fetch is needed)
- **Method:** Keyword-based detection
- **Keywords:**
  - `correlation`, `statistical analysis`, `statistics`
  - `standard deviation`, `variance`, `mean`
  - `distribution`, `skewness`, `kurtosis`
  - `outlier`, `outliers`, `percentile`, `quartile`
  - `time series`, `trend`, `seasonality`
  - `describe`, `summary`
- **Output:** `needs_pyodide` boolean (stored in state for later use)
- **Purpose:** Triggers simple SQL prompt to fetch raw data instead of performing analysis in database
- **Future Enhancement:** LLM-based classification with multilingual support

### 7. Chart Generation
- **Technology:** Plotly.js + react-plotly.js
- **Process:**
  1. Determine chart type from user question
  2. Extract x/y axes from SQL result columns
  3. Generate dynamic title from user question (60 char limit)
  4. Apply PII masking to data
  5. Return Plotly JSON spec to frontend

### 8. In-Browser Python Execution
- **Technology:** Pyodide (WebAssembly Python) + react-py
- **Libraries:** pandas (data manipulation)
- **Process:**
  1. LLM generates pandas analysis code
  2. Frontend loads Pyodide runtime
  3. Execute code in browser sandbox
  4. Display results in UI
- **Security:** No server-side code execution

### 9. Response Generation
- **Node:** `generate_response`
- **LLM Temperature:** 0.7 (natural, varied)
- **Output Format:** `<answer>` + `<insight>` (XML tags)
- **PII:** Already masked in data
- **Streaming:** Real-time response streaming to frontend

### 10. Error Feedback System (feedbacks.py) ğŸ†•
- **Purpose:** Provide LLM-specific hints for error correction
- **Functions:**
  - `get_unknown_tables_feedback()` - Invalid table names
  - `get_multiple_statements_feedback()` - Semicolon usage
  - `get_sql_comments_feedback()` - Comment removal
  - `get_forbidden_keyword_feedback()` - Dangerous keywords
  - `get_column_not_found_feedback()` - Case sensitivity
- **Example:** "Your previous attempt used invalid table 'it'. Use ONLY: customers, orders, products..."

---

## Data Flow

### User Question â†’ SQL Result

```
1. User Input
   "Show me top 5 customers by total spending"

2. Intent Classification (LLM)
   â†’ intent: "sql"

3. SQL Generation (LLM + Schema)
   schema_info.json â†’ Prompt
   â†’ SQL: SELECT c."name", SUM(o."amount") as total FROM customers c JOIN orders o...

4. SQL Validation (validation.py)
   âœ“ No dangerous keywords
   âœ“ Single statement
   âœ“ No comments
   âœ“ Tables exist: customers, orders

5. Query Execution (PostgreSQL)
   â†’ Result: [{"name": "Person #1", "total": 15000}, ...]
   â†’ PII masked: "John Smith" â†’ "Person #1"

6. Visualization Check (LLM)
   â†’ visualise: "no" (no chart keywords in question)

7. Pyodide Check (Keyword)
   â†’ needs_pyodide: false

8. Response Generation (LLM)
   â†’ Answer: "The top 5 customers by total spending are..."
   â†’ Insight: "Person #1 accounts for 30% of total revenue..."

9. Frontend Display
   â†’ Streaming text response to user
```

### Error Retry Flow (Updated 2026-01-09)

```
1. SQL Generation Attempt #1
   â†’ SQL: SELECT * FROM it WHERE ...

2. Validation Failed
   â†’ Error: "Unknown tables in query: {'it'}"
   â†’ Stored in query_result + messages (AIMessage)

3. execute_SQL (Skip)
   â†’ Detects error in query_result â†’ return {} (pass through)

4. check_query_validation
   â†’ is_error_result(query_result) â†’ True
   â†’ retry_count = 1 (from messages)
   â†’ return "retry"

5. SQL Generation Attempt #2 (with feedback)
   â†’ Previous error: "Unknown tables: {'it'}"
   â†’ Feedback: "Use ONLY: customers, orders, products. Do NOT abbreviate."
   â†’ SQL: SELECT * FROM customers WHERE ...

6. Validation Passed
   â†’ execute_SQL â†’ Success
```

---

## Dataset-Agnostic Design

### Core Principle
**All table/column information is loaded from `schema_info.json` at runtime, not hardcoded in prompts.**

### Implementation

1. **Schema Generation** (generate_schema.py)
   - Reads database metadata
   - Detects PII columns via LLM
   - Outputs `schema_info.json`

2. **Runtime Loading** (nodes.py:95-120)
   ```python
   def load_schema_info() -> str:
       global _SCHEMA_CACHE
       if _SCHEMA_CACHE is None:
           with open(SCHEMA_JSON_PATH, 'r') as f:
               schema_data = json.load(f)
           _SCHEMA_CACHE = format_schema_for_prompt(schema_data)
       return _SCHEMA_CACHE
   ```

3. **Prompt Injection** (prompts.py:85-175)
   ```python
   def get_sql_generation_prompt(schema_info: str, user_question: str) -> str:
       return f"""
       <database_schema>
       {schema_info}
       </database_schema>

       <user_question>
       {user_question}
       </user_question>
       """
   ```

### Benefits
- âœ… Swap datasets by replacing CSVs and re-running pipeline
- âœ… No code changes required for new schemas
- âœ… Scalable to different domains (retail, healthcare, finance, etc.)

---

## Module Responsibilities

### agent/ - LangGraph Workflow
- **graph.py:** StateGraph definition, conditional edges, retry logic
- **nodes.py:** Node implementations, LLM calls, error handling
- **prompts.py:** LLM prompt templates (initial generation)
- **feedbacks.py:** Error feedback messages (retry corrections)
- **state.py:** TypedDict schema for state management

### core/ - Shared Utilities
- **validation.py:** SQL security checks, table name validation
- **db.py:** Database connection pooling (singleton pattern)
- **logger.py:** Structured logging configuration
- **errors.py:** Custom exception hierarchy
- **console.py:** CLI output formatting

### data_pipeline/ - ETL
- **profiler.py:** Analyze CSV structure and statistics
- **relationship_discovery.py:** Suggest FK relationships (interactive)
- **load_data.py:** CSV â†’ PostgreSQL with constraints
- **integrity_checker.py:** Validate PK/FK integrity, detect offsets
- **transform_data.py:** Interactive SQL console for data fixes
- **pii_discovery.py:** LLM-based PII column detection
- **generate_schema.py:** Create schema metadata + PII report
- **setup.py:** Automated pipeline orchestration

---

## Performance Optimizations

### 1. Caching
- **Schema:** Loaded once, cached globally (`_SCHEMA_CACHE`)
- **Database Engine:** Connection pool reused (`_DB_ENGINE`)
- **Frontend:** Plotly chart memoized (React.memo)

### 2. Temperature Tuning
- **Intent (0.0):** Deterministic routing
- **SQL (0.1):** Accurate, minimal hallucination
- **Visualization (0.0):** Strict keyword matching
- **Response (0.7):** Natural, varied language

### 3. Streaming
- **LangGraph Server:** Real-time response streaming
- **Frontend:** Progressive UI updates

### 4. Connection Pooling
- **SQLAlchemy:** pool_size=5, max_overflow=10
- **Reuses connections** across requests

---

## Security Layers

### 1. SQL Injection Prevention (validation.py)
- Forbidden keyword blocking
- Multiple statement prevention
- Comment removal
- Table name whitelist validation

### 2. PII Masking (nodes.py:128-194)
- LLM-based detection during schema generation
- Deterministic masking at query execution
- Person names â†’ "Person #N" (sequential)
- Organization names preserved

### 3. Read-Only Access
- Only SELECT queries allowed
- No write operations (INSERT, UPDATE, DELETE)
- No schema modifications (CREATE, ALTER, DROP)

### 4. Rate Limiting
- Max retry limit: 3 attempts
- Prevents infinite loops

---

## Technology Choices

### Why LangGraph?
- **State Management:** Built-in state persistence
- **Conditional Routing:** Easy error handling with conditional edges
- **Streaming:** Native streaming support
- **Debugging:** LangSmith integration for trace visualization

### Why Cerebras (llama-3.3-70b)?
- **Performance:** Fast inference (previously Groq)
- **OpenAI-compatible API:** Easy integration
- **Cost-effective:** Competitive pricing

### Why PostgreSQL?
- **Relational:** Strong FK constraint support
- **JSON Support:** Native JSON column types
- **Mature:** Well-documented, stable

### Why Next.js 15?
- **App Router:** Server components, streaming
- **React 19:** Latest features (concurrent rendering)
- **TypeScript:** Type safety

---

## Related Documentation

- [Error Handling Guide](ERROR-HANDLING.md) - SQL validation, retry logic, debugging
- [Development Guide](DEVELOPMENT.md) - Contributing and extending *(coming soon)*
- [SQL Reference](sql.md) - SQL patterns and examples
- README.md - User setup and usage

---

**Last Updated:** 2026-01-10
**Version:** 1.1
**Recent Changes:**
- Workflow restructured: Pyodide classification now runs BEFORE SQL generation
- Added simple SQL prompt for pyodide-based analysis (no aggregations/functions)
- Fixed validation.py to skip Function/Parenthesis tokens (prevents false table extraction)
